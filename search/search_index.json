{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Lecture Notes on Deep Learning Use the navigation the right to find the corresponding lesson","title":"Home"},{"location":"#welcome-to-lecture-notes-on-deep-learning","text":"Use the navigation the right to find the corresponding lesson","title":"Welcome to Lecture Notes on Deep Learning"},{"location":"lesson-09/A1-pre-lesson/","text":"Before you get started 1. get a hugging face account Be sure to create a huggingface account, and visit this webpage: 2. get a hugging face API token Go to your profile and look under settings and then go to Access Tokens . 3. read + accept license (or you will be blocked) Here is the model card , the description + developer: https://huggingface.co/CompVis/stable-diffusion-v1-4 link You need to: read, accept, and acknowledge the LICENSE. if done correctly this will allow the following to be run: from diffusers import StableDiffusionPipeline pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) The above will pull the stable diffusion from the centralized huggingface modewl repository. This will also save the following information to a caching directory. This is passed during run_docker as an environmental variable. 4. Check your pytorch version import torch torch . __version__ The following means its pytorch intended for CUDA 10.2 which is old >>> 1.12.1+cu102 5. Common driver errors: you have a gpu but there's no NVIDIA drivers the container has CUDA==10.2 , but pytorch==cu11.3 , meaning the versions don't match 6. Can pytorch see the gpu? import torch print ( torch . cuda . is_available ()) >>> True Note: if the answer is False there is often a warning that is logged to give a hint at what is missing 7. is the GPU even available? $ nvidia-smi","title":"Pre Lesson"},{"location":"lesson-09/A1-pre-lesson/#before-you-get-started","text":"","title":"Before you get started"},{"location":"lesson-09/A1-pre-lesson/#1-get-a-hugging-face-account","text":"Be sure to create a huggingface account, and visit this webpage:","title":"1. get a hugging face account"},{"location":"lesson-09/A1-pre-lesson/#2-get-a-hugging-face-api-token","text":"Go to your profile and look under settings and then go to Access Tokens .","title":"2. get a hugging face API token"},{"location":"lesson-09/A1-pre-lesson/#3-read-accept-license-or-you-will-be-blocked","text":"Here is the model card , the description + developer: https://huggingface.co/CompVis/stable-diffusion-v1-4 link You need to: read, accept, and acknowledge the LICENSE. if done correctly this will allow the following to be run: from diffusers import StableDiffusionPipeline pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) The above will pull the stable diffusion from the centralized huggingface modewl repository. This will also save the following information to a caching directory. This is passed during run_docker as an environmental variable.","title":"3. read + accept license (or you will be blocked)"},{"location":"lesson-09/A1-pre-lesson/#4-check-your-pytorch-version","text":"import torch torch . __version__ The following means its pytorch intended for CUDA 10.2 which is old >>> 1.12.1+cu102","title":"4. Check your pytorch version"},{"location":"lesson-09/A1-pre-lesson/#5-common-driver-errors","text":"you have a gpu but there's no NVIDIA drivers the container has CUDA==10.2 , but pytorch==cu11.3 , meaning the versions don't match","title":"5. Common driver errors:"},{"location":"lesson-09/A1-pre-lesson/#6-can-pytorch-see-the-gpu","text":"import torch print ( torch . cuda . is_available ()) >>> True Note: if the answer is False there is often a warning that is logged to give a hint at what is missing","title":"6. Can pytorch see the gpu?"},{"location":"lesson-09/A1-pre-lesson/#7-is-the-gpu-even-available","text":"$ nvidia-smi","title":"7. is the GPU even available?"},{"location":"lesson-09/A2-stable-diffusion-fun/","text":"Stable Diffusion Introduction This is a combination of verbal notes + summarizations from this notebook walkthrough: github link: https://github.com/fastai/diffusion-nbs 1. About Hugging Face Pipelines, Generate your first image Can pass pipe() prompts and we can get some images out of it. from diffusers import StableDiffusionPipeline pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) # pass one prompt prompt = \"a photograph of an astronaut riding a horse\" pipe ( prompt ) . images [ 0 ] # can change the randomized seed, will generate same image dif torch . manual_seed ( 1024 ) pipe ( prompt ) . images [ 0 ] --image-- 1.1 What's happening step by step during image generation? The above images are generated around ~50 steps. starts with random noise slowly makes it less noisy to form it into an image And at a high level, thats how these models are created 1.2 Why not just 1 step? These models are not smart enough to do this in one-go. [ Caveat ] this is a very cutting-edge field with a lot of research, many of the new papers coming out see to shorten this step requirement to say 2-3 steps (instead of 51) 1.3 What happens with fewer steps? The images are not as clear, and appear to be more \"cloudy\" can \"convoluted\" 2. Concept: Guidance Scale Guidance Scale : What degree / intensity should focus be on the caption (words) vs. creating an image First, a little helper function to display multiple images, will use to show images along different steps def image_grid ( imgs , rows , cols ): \"\"\"For displaying a grid of images\"\"\" w , h = imgs [ 0 ] . size grid = Image . new ( 'RGB' , size = ( cols * w , rows * h )) for i , img in enumerate ( imgs ): grid . paste ( img , box = ( i % cols * w , i // cols * h )) return grid # first generate 4 images guidance_scale = [ 1.1 3 , 7 , 14 , ] num_rows , num_cols = 4 , 4 prompts = [ prompt ] * num_cols images = concat ( pipe ( prompts , guidance_scale = g ) . images for g in guidance_scale ) # then show 4 images image_grid ( images , rows = num_rows , cols = num_cols ) --image-- 2.1 What's happening behind the scenes? The algorithm is doing generating 2 images: image with the prompt image with no prompt Then it will average the two images together, and Guidance scale is a parameter that \"weights\" the average in a way 2.1.2 Subtraction instead of averaging Can subtract the two images instead of average and this can be accomplished with an additional function param: # a dog torch . manual_seed ( 1000 ) prompt = \"Labrador in the style of Vermeer\" pipe ( prompt ) . images [ 0 ] # remove a prompt for the provided caption torch . manual_seed ( 1000 ) pipe ( prompt , negative_prompt = \"blue\" ) . images [ 0 ] 3. Forget the captions, pass an image This can be thought of jump-starting right to the noisy image first. start with an image add some noise pre from diffusers import StableDiffusionImg2ImgPipeline from fastdownload import FastDownload pipe = StableDiffusionImg2ImgPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 , ) . to ( \"cuda\" ) # download an image to start with # VERY BASIC p = FastDownload () . download ( 'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png' ) init_image = Image . open ( p ) . convert ( \"RGB\" ) init_image --image-- Then the following code will use the random sketch as a starting point. The consideration factor is defined as strength=0.8 , meaning the algo should stick the original closely vs. exploring / altering the image torch . manual_seed ( 1000 ) prompt = \"Wolf howling at the moon, photorealistic 4K\" images = pipe ( prompt = prompt , num_images_per_prompt = 3 , init_image = init_image , strength = 0.8 , num_inference_steps = 50 ) . images image_grid ( images , rows = 1 , cols = 3 ) --image-- 3.1 Some advanced tricks: sketch -> generated image -> ANOTHER generated image start with the same sketch generate a decent image from the sketch starting point then use the output as the starting point again, with a different prompt # download an image to start with # VERY BASIC p = FastDownload () . download ( 'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png' ) # generate an image torch . manual_seed ( 1000 ) prompt = \"Wolf howling at the moon, photorealistic 4K\" images = pipe ( prompt = prompt , num_images_per_prompt = 3 , init_image = init_image , strength = 0.8 , num_inference_steps = 50 ) . images image_grid ( images , rows = 1 , cols = 3 ) # take the last image of generated set init_image = images [ 2 ] # and now apply a different prompt torch . manual_seed ( 1000 ) prompt = \"Oil painting of wolf howling at the moon by Van Gogh\" images = pipe ( prompt = prompt , num_images_per_prompt = 3 , init_image = init_image , strength = 1 , num_inference_steps = 70 ) . images image_grid ( images , rows = 1 , cols = 3 ) 3.1.1 Lambda Labs using the technique to make a text-2-pokemon model Lambda Labs Blog - Making the Pokemon Model In a nutshell: started with a database of pokemon images: huggingface link of pokemon images + captions then fine tuned one of the stable-diffusion models then it can take prompts to make pokemon!: Girl with a pearl earring Cute Obama creature Donald Trump Boris Johnson Totoro Hello Kitty --image--","title":"Stable Diffusion"},{"location":"lesson-09/A2-stable-diffusion-fun/#stable-diffusion-introduction","text":"This is a combination of verbal notes + summarizations from this notebook walkthrough: github link: https://github.com/fastai/diffusion-nbs","title":"Stable Diffusion Introduction"},{"location":"lesson-09/A2-stable-diffusion-fun/#1-about-hugging-face-pipelines-generate-your-first-image","text":"Can pass pipe() prompts and we can get some images out of it. from diffusers import StableDiffusionPipeline pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) # pass one prompt prompt = \"a photograph of an astronaut riding a horse\" pipe ( prompt ) . images [ 0 ] # can change the randomized seed, will generate same image dif torch . manual_seed ( 1024 ) pipe ( prompt ) . images [ 0 ] --image--","title":"1. About Hugging Face Pipelines, Generate your first image"},{"location":"lesson-09/A2-stable-diffusion-fun/#11-whats-happening-step-by-step-during-image-generation","text":"The above images are generated around ~50 steps. starts with random noise slowly makes it less noisy to form it into an image And at a high level, thats how these models are created","title":"1.1 What's happening step by step during image generation?"},{"location":"lesson-09/A2-stable-diffusion-fun/#12-why-not-just-1-step","text":"These models are not smart enough to do this in one-go. [ Caveat ] this is a very cutting-edge field with a lot of research, many of the new papers coming out see to shorten this step requirement to say 2-3 steps (instead of 51)","title":"1.2 Why not just 1 step?"},{"location":"lesson-09/A2-stable-diffusion-fun/#13-what-happens-with-fewer-steps","text":"The images are not as clear, and appear to be more \"cloudy\" can \"convoluted\"","title":"1.3 What happens with fewer steps?"},{"location":"lesson-09/A2-stable-diffusion-fun/#2-concept-guidance-scale","text":"Guidance Scale : What degree / intensity should focus be on the caption (words) vs. creating an image First, a little helper function to display multiple images, will use to show images along different steps def image_grid ( imgs , rows , cols ): \"\"\"For displaying a grid of images\"\"\" w , h = imgs [ 0 ] . size grid = Image . new ( 'RGB' , size = ( cols * w , rows * h )) for i , img in enumerate ( imgs ): grid . paste ( img , box = ( i % cols * w , i // cols * h )) return grid # first generate 4 images guidance_scale = [ 1.1 3 , 7 , 14 , ] num_rows , num_cols = 4 , 4 prompts = [ prompt ] * num_cols images = concat ( pipe ( prompts , guidance_scale = g ) . images for g in guidance_scale ) # then show 4 images image_grid ( images , rows = num_rows , cols = num_cols ) --image--","title":"2. Concept: Guidance Scale"},{"location":"lesson-09/A2-stable-diffusion-fun/#21-whats-happening-behind-the-scenes","text":"The algorithm is doing generating 2 images: image with the prompt image with no prompt Then it will average the two images together, and Guidance scale is a parameter that \"weights\" the average in a way","title":"2.1 What's happening behind the scenes?"},{"location":"lesson-09/A2-stable-diffusion-fun/#212-subtraction-instead-of-averaging","text":"Can subtract the two images instead of average and this can be accomplished with an additional function param: # a dog torch . manual_seed ( 1000 ) prompt = \"Labrador in the style of Vermeer\" pipe ( prompt ) . images [ 0 ] # remove a prompt for the provided caption torch . manual_seed ( 1000 ) pipe ( prompt , negative_prompt = \"blue\" ) . images [ 0 ]","title":"2.1.2 Subtraction instead of averaging"},{"location":"lesson-09/A2-stable-diffusion-fun/#3-forget-the-captions-pass-an-image","text":"This can be thought of jump-starting right to the noisy image first. start with an image add some noise pre from diffusers import StableDiffusionImg2ImgPipeline from fastdownload import FastDownload pipe = StableDiffusionImg2ImgPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 , ) . to ( \"cuda\" ) # download an image to start with # VERY BASIC p = FastDownload () . download ( 'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png' ) init_image = Image . open ( p ) . convert ( \"RGB\" ) init_image --image-- Then the following code will use the random sketch as a starting point. The consideration factor is defined as strength=0.8 , meaning the algo should stick the original closely vs. exploring / altering the image torch . manual_seed ( 1000 ) prompt = \"Wolf howling at the moon, photorealistic 4K\" images = pipe ( prompt = prompt , num_images_per_prompt = 3 , init_image = init_image , strength = 0.8 , num_inference_steps = 50 ) . images image_grid ( images , rows = 1 , cols = 3 ) --image--","title":"3. Forget the captions, pass an image"},{"location":"lesson-09/A2-stable-diffusion-fun/#31-some-advanced-tricks-sketch-generated-image-another-generated-image","text":"start with the same sketch generate a decent image from the sketch starting point then use the output as the starting point again, with a different prompt # download an image to start with # VERY BASIC p = FastDownload () . download ( 'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png' ) # generate an image torch . manual_seed ( 1000 ) prompt = \"Wolf howling at the moon, photorealistic 4K\" images = pipe ( prompt = prompt , num_images_per_prompt = 3 , init_image = init_image , strength = 0.8 , num_inference_steps = 50 ) . images image_grid ( images , rows = 1 , cols = 3 ) # take the last image of generated set init_image = images [ 2 ] # and now apply a different prompt torch . manual_seed ( 1000 ) prompt = \"Oil painting of wolf howling at the moon by Van Gogh\" images = pipe ( prompt = prompt , num_images_per_prompt = 3 , init_image = init_image , strength = 1 , num_inference_steps = 70 ) . images image_grid ( images , rows = 1 , cols = 3 )","title":"3.1 Some advanced tricks: sketch -&gt; generated image -&gt; ANOTHER generated image"},{"location":"lesson-09/A2-stable-diffusion-fun/#311-lambda-labs-using-the-technique-to-make-a-text-2-pokemon-model","text":"Lambda Labs Blog - Making the Pokemon Model In a nutshell: started with a database of pokemon images: huggingface link of pokemon images + captions then fine tuned one of the stable-diffusion models then it can take prompts to make pokemon!: Girl with a pearl earring Cute Obama creature Donald Trump Boris Johnson Totoro Hello Kitty --image--","title":"3.1.1 Lambda Labs using the technique to make a text-2-pokemon model"},{"location":"lesson-09/A3-fine-tuning/","text":"4. Fine Tuning There are many ways of fine-tuning, but can take a lot of data + time. One is called Textual inversion 4.1 Fine Tuning: Textual Inversion Lets say we want to create a new style for indian watercolor portraits. Art samples can be found here: https://huggingface.co/sd-concepts-library/indian-watercolor-portraits . Let's call it <watercolor-properties> 4.1.1 Download a 1 new word embedding for our new style name from diffusers import StableDiffusionPipeline from fastdownload import FastDownload pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 ) pipe = pipe . to ( \"cuda\" ) # will download a single single embedding embeds_url = \"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin\" embeds_path = FastDownload () . download ( embeds_url ) embeds_dict = torch . load ( str ( embeds_path ), map_location = \"cpu\" ) \"\"\" embeds_dict { \"<watercolor-portrait\">: torch.tensor[float32, size=768] } \"\"\" 4.1.2 Add the new token + insert the downloaded embedding # from the pipe, access the tokenizer tokenizer = pipe . tokenizer # from the pipe access the text_encoder text_encoder = pipe . text_encoder # a str, and a torch tensor new_token , embeds = next ( iter ( embeds_dict . items ())) embeds = embeds . to ( text_encoder . dtype ) # this was the key in the embed_dict new_token # >>> \"<watercolor-portrait\"> # add new token text_encoder . resize_token_embeddings ( len ( tokenizer )) new_token_id = tokenizer . convert_tokens_to_ids ( new_token ) # insert the embedding for our new token text_encoder . get_input_embeddings () . weight . data [ new_token_id ] = embeds --image-- 4.2 Fine Tuning: DreamBooth Take a rare word in the existing vocabulary, then provide it images and fine tune the model. In this case a model was fine tuned with Jeremy Howard (the instructor) associated to the term sks from diffusers import StableDiffusionPipeline from fastdownload import FastDownload # downloading a model someone else finetuned pipe = StableDiffusionPipeline . from_pretrained ( \"pcuenq/jh_dreambooth_1000\" , revision = \"fp16\" , torch_dtype = torch . float16 ) pipe = pipe . to ( \"cuda\" ) torch . manual_seed ( 1000 ) prompt = \"Painting of sks person in the style of Paul Signac\" images = pipe ( prompt , num_images_per_prompt = 4 ) . images image_grid ( images , 1 , 4 ) --image--","title":"Fine Tuning Stable Diffusion"},{"location":"lesson-09/A3-fine-tuning/#4-fine-tuning","text":"There are many ways of fine-tuning, but can take a lot of data + time. One is called Textual inversion","title":"4. Fine Tuning"},{"location":"lesson-09/A3-fine-tuning/#41-fine-tuning-textual-inversion","text":"Lets say we want to create a new style for indian watercolor portraits. Art samples can be found here: https://huggingface.co/sd-concepts-library/indian-watercolor-portraits . Let's call it <watercolor-properties>","title":"4.1 Fine Tuning: Textual Inversion"},{"location":"lesson-09/A3-fine-tuning/#411-download-a-1-new-word-embedding-for-our-new-style-name","text":"from diffusers import StableDiffusionPipeline from fastdownload import FastDownload pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , revision = \"fp16\" , torch_dtype = torch . float16 ) pipe = pipe . to ( \"cuda\" ) # will download a single single embedding embeds_url = \"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin\" embeds_path = FastDownload () . download ( embeds_url ) embeds_dict = torch . load ( str ( embeds_path ), map_location = \"cpu\" ) \"\"\" embeds_dict { \"<watercolor-portrait\">: torch.tensor[float32, size=768] } \"\"\"","title":"4.1.1 Download a 1 new word embedding for our new style name"},{"location":"lesson-09/A3-fine-tuning/#412-add-the-new-token-insert-the-downloaded-embedding","text":"# from the pipe, access the tokenizer tokenizer = pipe . tokenizer # from the pipe access the text_encoder text_encoder = pipe . text_encoder # a str, and a torch tensor new_token , embeds = next ( iter ( embeds_dict . items ())) embeds = embeds . to ( text_encoder . dtype ) # this was the key in the embed_dict new_token # >>> \"<watercolor-portrait\"> # add new token text_encoder . resize_token_embeddings ( len ( tokenizer )) new_token_id = tokenizer . convert_tokens_to_ids ( new_token ) # insert the embedding for our new token text_encoder . get_input_embeddings () . weight . data [ new_token_id ] = embeds --image--","title":"4.1.2 Add the new token + insert the downloaded embedding"},{"location":"lesson-09/A3-fine-tuning/#42-fine-tuning-dreambooth","text":"Take a rare word in the existing vocabulary, then provide it images and fine tune the model. In this case a model was fine tuned with Jeremy Howard (the instructor) associated to the term sks from diffusers import StableDiffusionPipeline from fastdownload import FastDownload # downloading a model someone else finetuned pipe = StableDiffusionPipeline . from_pretrained ( \"pcuenq/jh_dreambooth_1000\" , revision = \"fp16\" , torch_dtype = torch . float16 ) pipe = pipe . to ( \"cuda\" ) torch . manual_seed ( 1000 ) prompt = \"Painting of sks person in the style of Paul Signac\" images = pipe ( prompt , num_images_per_prompt = 4 ) . images image_grid ( images , 1 , 4 ) --image--","title":"4.2 Fine Tuning: DreamBooth"},{"location":"lesson-09/B1-under-the-hood/","text":"What is Stable Diffusion? First these image generating processes are not a perfect process. Jeremy tried to tune a model for a teddy bear \"tiny\" riding a horse, and was unsuccessful. Similarly even teddy bears on pink rug, also did not produce the expected images. Concept: Stable Diffusion to generate handwritten digits For a conceptual example, let's consider the following: Make a hand-written digit image generator imagine some function f(x) that if you fed it an image it could give you a probability P(x) = 0.97 meaning a image was a hand-written digit if the image is very noisy, then the P(x) = 0.02 would be very low Premise : if a function like this exists, this f(x) could be used to generate images What happens if the image is \"tweaked\"? Now consider the low scoring image X3 . If a few pixels are turned darker, or lighter - maybe to form a 7 or leg of 4 , This will in turn also increase the P(X) that the image is of a hand-written image considering pixel by pixel tweaks and changes, this is close to calculating the gradient of the probability over the pixels If we had the gradient, what would we do with it? if we could get the gradient, we could subtract it from the original image, meaning the image would slowly become more like a \"real\" hand-written number in deep-learning the gradient is changing the weights of the model. In stable diffusion we are changing the pixels of the image Start with an image subtract the gradient (will talk about how we get this later) times some C constant image will be updated (ideally closer to a real number) the probability is re-evaluated, would probably go up by a small bit How to calculate the gradient? just like in high-school calculus, each pixel's gradient could be calculated for MNIST images, this is 28 x 28 == 784px , which is a lot of calculations to do, very slow it would be much better if there existed a f.backward() function that would provide us the grad_X3 But first, what is our f going to be? Hint: a neural-network High-level problem definition lets assume our NN problem definition will be the following: predict how much noise is in an image Why? because quantifying \"similarity to a hand-written number\" is very difficult Need to define inputs outputs loss function A forward-cycle inputs: IMG + N(sigma, mu) , in this case messy images outputs: N_hat(sigma, mu) predicted noise, in this case an image of ONLY noise loss function: MSE calculate the gradient from the MSELoss.backward() subtract gradient from IMG - grad and repeat! Sample cycle, a noisy image coes in, the NN predicts what it thinks is noise, we subtract it, and the image gets closer to looking like a 4 So... how to add noise? MNIST images are very tiny + only black in white. Normal images are 512 x 512 x 3 = 6MB! (last one is color channels RGB). This data \"volume\" will be difficult to process, how can it be done more efficiently? If the internet is considered, there are many full-sized images floating around that don't take the full space HD image (1920 x 1080 x 3) = 6MB !, and there are plenty of images much smaller 100KB Hint: think about JPEGs ... image compression! Introducing autoencoders The original volume is on the left, 512 x 512 x 3 , if we do the following: [INPUT] IMG_RAW (512 x 512 x 3) -> ~6,000,000 <conv, stride 2> -> (256 x 256 x 6) <conv, stride 2> -> (128 x 128 x 12) <conv, stride 2> -> (64 x 24 x 24) <resnet blocks> -> (64 x 4 x 4) -> 16,000 (much smaller) <resnet blocks> -> (64 x 24 x 24) <conv, stride 2> -> (128 x 128 x 12) <conv, stride 2> -> (256 x 256 x 6) <conv, stride 2> -> [OUTPUT] IMG_RAW (512 x 512 x 3) Its a very strange NN design, that keeps shrinking the dimensions to a point via convolutions, then does the reverse, expanding with other convolutions till the output is ... the same image. Inputs: Image sized 512 x 512 x 3 Outputs: Image sized 512 x 512 x 3 What's the point of this? The main point of architectures like this is to \"compress\" complex inputs into smaller vector spaces. it can be for data storage size reasons it can be for extremely sparse inputs it could be for very correlated inputs, aka a non-linear PCA approach Anything that Shrinks the inputs, or the first half is called the encoder . Anything that expands the size is the decoder . So imagine, if our entire audience had the same trained + fit decoder , then \"compressed\" data files could be passed around instead of the full-sized one. We already do this today with JPEGS. Since the loss function is basically \"is this the same image\", MSE will be used again to calculate the pixel vs pixel difference. This is trained over a large corpus of images Assembling the compressor + expander with the NN first the auto-encoder will be trained independently once its fit, it is dis-assembled into encoder and decoder parts Raw Images are compressed in the encoder into latents Latents are fed into the NN aka Unet which will predict noise-only-latents noise-only-latents will be expanded into the full image size Caveat: this auto-encoder approach is optional if you have infinite compute, but it is very resource hungry What about captions? We've talked about how images are created, but what about the prompt astronaut riding a horse how does the text fit into the image creation? In the handwritten numbers case, there's only 10 possible digits, so a 1-hot vector (size 10) could be passed to the NN to give the model more information about what noise to target. The main issue with this is there are too many phrases in the english language to make a 1-hot vector large enough. So another approach is needed to do the following: astronaut on a horse -> a vector football in the style of monet -> different vector Captions to vectors: an example Let's say there are 3 images: - dinosaur - bird - flaming sword From the internet, many of these images have short text phrases describing them for accessibility purposes (people who have a hard time seeing, there's text descriptions). A sample HTML code would look like the following: < img src = \"/myweb/dino.jpg\" alt = \"a hungry dinosaur\" > < img src = \"/myweb/bird.jpg\" alt = \"a soaring bird\" > < img src = \"/myweb/sword.jpg\" alt = \"a burning sword\" > Now that images + phrases have be associated, we can make a siamese-net like approach. the text will have its own encoder, say (1 x 64) the image will have its own encoder say (1 x 64) the text_vectors and the image_vectors will be dot-product with each other to give a closeness score. 0.95 means they are related 0.01 would mean the image + the caption are not related the loss function will consider both correct answers and incorrect answers dino + \"a hungry dinosaur\" should be a high score sword + \"a soaring bird\" should be a low score and through this both the text_encoder and the image_encoder will be trained How to pick your randomness","title":"Under the Hood"},{"location":"lesson-09/B1-under-the-hood/#what-is-stable-diffusion","text":"First these image generating processes are not a perfect process. Jeremy tried to tune a model for a teddy bear \"tiny\" riding a horse, and was unsuccessful. Similarly even teddy bears on pink rug, also did not produce the expected images.","title":"What is Stable Diffusion?"},{"location":"lesson-09/B1-under-the-hood/#concept-stable-diffusion-to-generate-handwritten-digits","text":"For a conceptual example, let's consider the following: Make a hand-written digit image generator imagine some function f(x) that if you fed it an image it could give you a probability P(x) = 0.97 meaning a image was a hand-written digit if the image is very noisy, then the P(x) = 0.02 would be very low Premise : if a function like this exists, this f(x) could be used to generate images","title":"Concept: Stable Diffusion to generate handwritten digits"},{"location":"lesson-09/B1-under-the-hood/#what-happens-if-the-image-is-tweaked","text":"Now consider the low scoring image X3 . If a few pixels are turned darker, or lighter - maybe to form a 7 or leg of 4 , This will in turn also increase the P(X) that the image is of a hand-written image considering pixel by pixel tweaks and changes, this is close to calculating the gradient of the probability over the pixels","title":"What happens if the image is \"tweaked\"?"},{"location":"lesson-09/B1-under-the-hood/#if-we-had-the-gradient-what-would-we-do-with-it","text":"if we could get the gradient, we could subtract it from the original image, meaning the image would slowly become more like a \"real\" hand-written number in deep-learning the gradient is changing the weights of the model. In stable diffusion we are changing the pixels of the image Start with an image subtract the gradient (will talk about how we get this later) times some C constant image will be updated (ideally closer to a real number) the probability is re-evaluated, would probably go up by a small bit","title":"If we had the gradient, what would we do with it?"},{"location":"lesson-09/B1-under-the-hood/#how-to-calculate-the-gradient","text":"just like in high-school calculus, each pixel's gradient could be calculated for MNIST images, this is 28 x 28 == 784px , which is a lot of calculations to do, very slow it would be much better if there existed a f.backward() function that would provide us the grad_X3 But first, what is our f going to be? Hint: a neural-network","title":"How to calculate the gradient?"},{"location":"lesson-09/B1-under-the-hood/#high-level-problem-definition","text":"lets assume our NN problem definition will be the following: predict how much noise is in an image Why? because quantifying \"similarity to a hand-written number\" is very difficult Need to define inputs outputs loss function","title":"High-level problem definition"},{"location":"lesson-09/B1-under-the-hood/#a-forward-cycle","text":"inputs: IMG + N(sigma, mu) , in this case messy images outputs: N_hat(sigma, mu) predicted noise, in this case an image of ONLY noise loss function: MSE calculate the gradient from the MSELoss.backward() subtract gradient from IMG - grad and repeat! Sample cycle, a noisy image coes in, the NN predicts what it thinks is noise, we subtract it, and the image gets closer to looking like a 4","title":"A forward-cycle"},{"location":"lesson-09/B1-under-the-hood/#so-how-to-add-noise","text":"MNIST images are very tiny + only black in white. Normal images are 512 x 512 x 3 = 6MB! (last one is color channels RGB). This data \"volume\" will be difficult to process, how can it be done more efficiently? If the internet is considered, there are many full-sized images floating around that don't take the full space HD image (1920 x 1080 x 3) = 6MB !, and there are plenty of images much smaller 100KB Hint: think about JPEGs ... image compression!","title":"So... how to add noise?"},{"location":"lesson-09/B1-under-the-hood/#introducing-autoencoders","text":"The original volume is on the left, 512 x 512 x 3 , if we do the following: [INPUT] IMG_RAW (512 x 512 x 3) -> ~6,000,000 <conv, stride 2> -> (256 x 256 x 6) <conv, stride 2> -> (128 x 128 x 12) <conv, stride 2> -> (64 x 24 x 24) <resnet blocks> -> (64 x 4 x 4) -> 16,000 (much smaller) <resnet blocks> -> (64 x 24 x 24) <conv, stride 2> -> (128 x 128 x 12) <conv, stride 2> -> (256 x 256 x 6) <conv, stride 2> -> [OUTPUT] IMG_RAW (512 x 512 x 3) Its a very strange NN design, that keeps shrinking the dimensions to a point via convolutions, then does the reverse, expanding with other convolutions till the output is ... the same image. Inputs: Image sized 512 x 512 x 3 Outputs: Image sized 512 x 512 x 3 What's the point of this? The main point of architectures like this is to \"compress\" complex inputs into smaller vector spaces. it can be for data storage size reasons it can be for extremely sparse inputs it could be for very correlated inputs, aka a non-linear PCA approach Anything that Shrinks the inputs, or the first half is called the encoder . Anything that expands the size is the decoder . So imagine, if our entire audience had the same trained + fit decoder , then \"compressed\" data files could be passed around instead of the full-sized one. We already do this today with JPEGS. Since the loss function is basically \"is this the same image\", MSE will be used again to calculate the pixel vs pixel difference. This is trained over a large corpus of images","title":"Introducing autoencoders"},{"location":"lesson-09/B1-under-the-hood/#assembling-the-compressor-expander-with-the-nn","text":"first the auto-encoder will be trained independently once its fit, it is dis-assembled into encoder and decoder parts Raw Images are compressed in the encoder into latents Latents are fed into the NN aka Unet which will predict noise-only-latents noise-only-latents will be expanded into the full image size Caveat: this auto-encoder approach is optional if you have infinite compute, but it is very resource hungry","title":"Assembling the compressor + expander with the NN"},{"location":"lesson-09/B1-under-the-hood/#what-about-captions","text":"We've talked about how images are created, but what about the prompt astronaut riding a horse how does the text fit into the image creation? In the handwritten numbers case, there's only 10 possible digits, so a 1-hot vector (size 10) could be passed to the NN to give the model more information about what noise to target. The main issue with this is there are too many phrases in the english language to make a 1-hot vector large enough. So another approach is needed to do the following: astronaut on a horse -> a vector football in the style of monet -> different vector","title":"What about captions?"},{"location":"lesson-09/B1-under-the-hood/#captions-to-vectors-an-example","text":"Let's say there are 3 images: - dinosaur - bird - flaming sword From the internet, many of these images have short text phrases describing them for accessibility purposes (people who have a hard time seeing, there's text descriptions). A sample HTML code would look like the following: < img src = \"/myweb/dino.jpg\" alt = \"a hungry dinosaur\" > < img src = \"/myweb/bird.jpg\" alt = \"a soaring bird\" > < img src = \"/myweb/sword.jpg\" alt = \"a burning sword\" > Now that images + phrases have be associated, we can make a siamese-net like approach. the text will have its own encoder, say (1 x 64) the image will have its own encoder say (1 x 64) the text_vectors and the image_vectors will be dot-product with each other to give a closeness score. 0.95 means they are related 0.01 would mean the image + the caption are not related the loss function will consider both correct answers and incorrect answers dino + \"a hungry dinosaur\" should be a high score sword + \"a soaring bird\" should be a low score and through this both the text_encoder and the image_encoder will be trained How to pick your randomness","title":"Captions to vectors: an example"},{"location":"lesson-10/A1-Intro/","text":"Lesson 10 Puru - a bunch of images of (spherical) linear interpolation between two different starting points Student examples of making different pictures Going from an old car prompt to a new car prompt dinosaur -> to a bird dog -> unicorn classic optimizer efforts, couple of hours Recap With Handwritten digits, start with a 7, add some noise and get a \"noisy 7\" and feed this into Unet, and try to predict the noise we can also pass in the actual number 7 as \"guidance\" to help it along Turn captions into embeddings, via using existing images + captions (alt tab) Then train a text encoder + image encoder compare the dot product contrastive loss take the text encoder and feed into --image-- Doing inference put a prompt + some noise Unet will predict noise will subtract gradually, and then resubmit the updated photo used to take 1000 steps, now takes 60 (maybe less!) --image-- Papers A lot of interest + work still being focused on this work. Progressive Distillation for Fast Sampling of Diffusion Models On Distillation of guided Diffusion Models Imagic: Text-Based Real Image Editing with Diffusion Models Overview of \"Progressive Distillation for Fast Sampling of Diffusion Models\" Hand-written Notes","title":"Intro"},{"location":"lesson-10/A1-Intro/#lesson-10","text":"Puru - a bunch of images of (spherical) linear interpolation between two different starting points Student examples of making different pictures Going from an old car prompt to a new car prompt dinosaur -> to a bird dog -> unicorn classic optimizer efforts, couple of hours","title":"Lesson 10"},{"location":"lesson-10/A1-Intro/#recap","text":"With Handwritten digits, start with a 7, add some noise and get a \"noisy 7\" and feed this into Unet, and try to predict the noise we can also pass in the actual number 7 as \"guidance\" to help it along Turn captions into embeddings, via using existing images + captions (alt tab) Then train a text encoder + image encoder compare the dot product contrastive loss take the text encoder and feed into --image--","title":"Recap"},{"location":"lesson-10/A1-Intro/#doing-inference","text":"put a prompt + some noise Unet will predict noise will subtract gradually, and then resubmit the updated photo used to take 1000 steps, now takes 60 (maybe less!) --image--","title":"Doing inference"},{"location":"lesson-10/A1-Intro/#papers","text":"A lot of interest + work still being focused on this work. Progressive Distillation for Fast Sampling of Diffusion Models On Distillation of guided Diffusion Models Imagic: Text-Based Real Image Editing with Diffusion Models","title":"Papers"},{"location":"lesson-10/A1-Intro/#overview-of-progressive-distillation-for-fast-sampling-of-diffusion-models","text":"Hand-written Notes","title":"Overview of \"Progressive Distillation for Fast Sampling of Diffusion Models\""},{"location":"lesson-10/A2-pipeline-under-the-hood/","text":"Looking inside the Pipeline of Hugging Face How do we get to the following? from diffusers import StableDiffusionPipeline To get started at a high level, let's download some the key components needed for assembling everything CLIPTextModel : turn captions into vectors CLIPTokenizer : AutoencoderKL : how to squish our image down to a manageable size UNet2DConditionModel : the unet model from transformers import CLIPTextModel , CLIPTokenizer from diffusers import AutoencoderKL , UNet2DConditionModel from diffusers import LMSDiscreteScheduler 1. Get the different components Similar to the huggingface pattern, will download the different parts vae = AutoencoderKL . from_pretrained ( \"stabilityai/sd-vae-ft-ema\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) unet = UNet2DConditionModel . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , subfolder = \"unet\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) Next we need a noise generator, or level of noise intensity generator. Will use the following relationship The following is a scheduler is from Katherine Crowson https://github.com/crowsonkb a AI/Generative artist beta_start , beta_end = 0.00085 , 0.012 plt . plot ( torch . linspace ( beta_start ** 0.5 , beta_end ** 0.5 , 1000 ) ** 2 ) plt . xlabel ( 'Timestep' ) plt . ylabel ( '\u03b2' ); --image-- scheduler = LMSDiscreteScheduler ( beta_start = beta_start , beta_end = beta_end , beta_schedule = \"scaled_linear\" , num_train_timesteps = 1000 ) 2. Lets generate the astronaut on a horse again Here's the settings that we will use prompt = [ \"a photograph of an astronaut riding a horse\" ] height = 512 width = 512 num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1 text_input = tokenizer ( prompt , padding = \"max_length\" , max_length = tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" ) Looking at the tokenized text: note that the 49407 represents padding to ensure all inputs are the same length. text_input [ 'input_ids' ] # torch.Size([1, 77]) # \"a photograph of an astronaut riding a horse\" >>> tensor ([[ 49406 , 320 , 8853 , 539 , 550 , 18376 , 6765 , 320 , 4558 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 ]]) tokenizer . decode ( 49407 ) >>> \"<|endoftext|>\" In addition, the text_input also has an attention mask to ignore these \"EOT\" tokens: text_input [ \"attention_mask\" ] # torch.Size([1, 77]) # \"a photograph of an astronaut riding a horse\" >>> tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]) Now one the text is run through the encoder, the appropriate embeddings are mapped over text_embeddings = text_encoder ( text_input . input_ids . to ( \"cuda\" ))[ 0 ] . half () text_embeddings . shape # >>> torch.Size([1, 77, 768]) From our discussion before, we need to generate a un-guided photo, but as a result, still need to pass empty inputs to the encoder Note that .half() means floating point 16, or fp16 or float16 which trades off decimal precision for size # ensure the input is the same length max_length = text_input . input_ids . shape [ - 1 ] # the token will be the empty string uncond_input = tokenizer ( [ \"\" ] * batch_size , padding = \"max_length\" , max_length = max_length , return_tensors = \"pt\" ) # will get the same size, but the embeddings will the the same uncond_embeddings = text_encoder ( uncond_input . input_ids . to ( \"cuda\" ))[ 0 ] . half () uncond_embeddings . shape # >>> torch.Size([1, 77, 768]) # combine both inputs together for processing purposes text_embeddings = torch . cat ([ uncond_embeddings , text_embeddings ]) normal gaussian noise is generated, and note that the noise generated is at the compressed level, not at the original size: original size: 512px x 512px x 3 compressed size: 64px x 64px x 4 # lets create some noise torch . manual_seed ( 100 ) latents = torch . randn (( batch_size , unet . in_channels , height // 8 , width // 8 )) latents = latents . to ( \"cuda\" ) . half () latents . shape \"\"\" >>> torch.Size([1, 4, 64, 64]) tensor([[[[ 0.1268, 1.3564, 0.5630, ..., -0.8042, -0.6245, -0.5884], [ 1.6699, -0.9272, -0.9761, ..., 2.3848, -0.0355, -0.3179], [ 0.3579, -1.7842, -0.3052, ..., 0.4880, -2.5781, 1.0010], ... \"\"\" # set the scheduler - currently 70 steps scheduler . set_timesteps ( num_inference_steps ) # tensor = tensor * tensor(14.6146) as an example latents = latents * scheduler . init_noise_sigma \"\"\" Normally would be a full 1000 steps, but our methods will be skipping a bit To look at the approximations. Its not really a \"timestep\" it is just means how much noise scheduler.timesteps >>> tensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304, 897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826, 796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348, ... for 70 steps ... 0.0000] scheduler.sigmas >>> tensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301, 9.6279, 8.9020, 8.2443, 7.6472, 7.1044, 6.6102, 6.1594, 5.7477, 5.3709, 5.0258, 4.7090, 4.4178, 4.1497, 3.9026, 3.6744, 3.4634, 3.2680, 3.0867, 2.9183, ... for 70 steps .... 0.000] Reminder that sigma is the 'variance' or the amount of noise, that is decreasing, starting with lots of noise and decreasing it \"\"\" from tqdm.auto import tqdm # loop through decreasing amounts of noise for i , t in enumerate ( tqdm ( scheduler . timesteps )): input = torch . cat ([ latents ] * 2 ) input = scheduler . scale_model_input ( input , t ) # predict the noise residual with torch . no_grad (): # remember the text_embeddings is [empty embeddings, prompt embeddings] # - passes some initial noise (input), # - a noise constant t, and our text ebmeddings input pred = unet ( input , t , encoder_hidden_states = text_embeddings ) . sample # pred is torch.Size([1, 4, 64, 64]) # perform guidance pred_uncond , pred_text = pred . chunk ( 2 ) # this takes the starting picture + adds some small part of the guidance pred = pred_uncond + guidance_scale * ( pred_text - pred_uncond ) # compute the \"previous\" noisy sample # the latents are updated and then will be fed in again latents = scheduler . step ( pred , t , latents ) . prev_sample \"\"\" Once this is complete, decompress the image with the VAE to get generated + adjusted photo, then feed it through PIL to take a look \"\"\" with torch . no_grad (): image = vae . decode ( 1 / 0.18215 * latents ) . sample # 0.18215 - is a constant determined by the paper # image.shape -> torch.Size([1, 3, 512, 512]) image = ( image / 2 + 0.5 ) . clamp ( 0 , 1 ) image = image [ 0 ] . detach () . cpu () . permute ( 1 , 2 , 0 ) . numpy () image = ( image * 255 ) . round () . astype ( \"uint8\" ) Image . fromarray ( image ) 3. Refactoring for programming agility The following code is the same as above, but will allow for reusability def text_enc ( prompts : List [ str ], maxlen : int = None ) -> torch . TensorType : \"\"\"Encodes a list of strings Returns: torch.TensorType[torch.Size([1, 77, 768]), float16] \"\"\" if maxlen is None : maxlen = tokenizer . model_max_length inp = tokenizer ( prompts , padding = \"max_length\" , max_length = maxlen , truncation = True , return_tensors = \"pt\" ) return text_encoder ( inp . input_ids . to ( \"cuda\" ))[ 0 ] . half () def mk_img ( t ): \"\"\"transforms raw decompressed image into PIL-compatible ranges + sizes\"\"\" image = ( t / 2 + 0.5 ) . clamp ( 0 , 1 ) . detach () . cpu () . permute ( 1 , 2 , 0 ) . numpy () return Image . fromarray (( image * 255 ) . round () . astype ( \"uint8\" )) def mk_samples ( prompts : List [ str ], g = 7.5 , seed = 100 , steps = 70 ): \"\"\"infers based on Unet, and returns a VAE decoded image of torch.Size([1, 3, 512, 512])\"\"\" bs = len ( prompts ) text = text_enc ( prompts ) uncond = text_enc ([ \"\" ] * bs , text . shape [ 1 ]) emb = torch . cat ([ uncond , text ]) if seed : torch . manual_seed ( seed ) latents = torch . randn (( bs , unet . in_channels , height // 8 , width // 8 )) scheduler . set_timesteps ( steps ) latents = latents . to ( \"cuda\" ) . half () * scheduler . init_noise_sigma for i , ts in enumerate ( tqdm ( scheduler . timesteps )): inp = scheduler . scale_model_input ( torch . cat ([ latents ] * 2 ), ts ) with torch . no_grad (): u , t = unet ( inp , ts , encoder_hidden_states = emb ) . sample . chunk ( 2 ) pred = u + g * ( t - u ) latents = scheduler . step ( pred , ts , latents ) . prev_sample with torch . no_grad (): return vae . decode ( 1 / 0.18215 * latents ) . sample And now can run some samples in only a few lines: prompts = [ \"a photograph of an astronaut riding a horse\" , \"an oil painting of an astronaut riding a horse in the style of grant wood\" ] # will run the exact process above images = mk_samples ( prompts ) Homework try and condense Image2Image, Negative Prompts, to really understand the code","title":"Under the hood of Pipeline"},{"location":"lesson-10/A2-pipeline-under-the-hood/#looking-inside-the-pipeline-of-hugging-face","text":"How do we get to the following? from diffusers import StableDiffusionPipeline To get started at a high level, let's download some the key components needed for assembling everything CLIPTextModel : turn captions into vectors CLIPTokenizer : AutoencoderKL : how to squish our image down to a manageable size UNet2DConditionModel : the unet model from transformers import CLIPTextModel , CLIPTokenizer from diffusers import AutoencoderKL , UNet2DConditionModel from diffusers import LMSDiscreteScheduler","title":"Looking inside the Pipeline of Hugging Face"},{"location":"lesson-10/A2-pipeline-under-the-hood/#1-get-the-different-components","text":"Similar to the huggingface pattern, will download the different parts vae = AutoencoderKL . from_pretrained ( \"stabilityai/sd-vae-ft-ema\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) unet = UNet2DConditionModel . from_pretrained ( \"CompVis/stable-diffusion-v1-4\" , subfolder = \"unet\" , torch_dtype = torch . float16 ) . to ( \"cuda\" ) Next we need a noise generator, or level of noise intensity generator. Will use the following relationship The following is a scheduler is from Katherine Crowson https://github.com/crowsonkb a AI/Generative artist beta_start , beta_end = 0.00085 , 0.012 plt . plot ( torch . linspace ( beta_start ** 0.5 , beta_end ** 0.5 , 1000 ) ** 2 ) plt . xlabel ( 'Timestep' ) plt . ylabel ( '\u03b2' ); --image-- scheduler = LMSDiscreteScheduler ( beta_start = beta_start , beta_end = beta_end , beta_schedule = \"scaled_linear\" , num_train_timesteps = 1000 )","title":"1. Get the different components"},{"location":"lesson-10/A2-pipeline-under-the-hood/#2-lets-generate-the-astronaut-on-a-horse-again","text":"Here's the settings that we will use prompt = [ \"a photograph of an astronaut riding a horse\" ] height = 512 width = 512 num_inference_steps = 70 guidance_scale = 7.5 batch_size = 1 text_input = tokenizer ( prompt , padding = \"max_length\" , max_length = tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" ) Looking at the tokenized text: note that the 49407 represents padding to ensure all inputs are the same length. text_input [ 'input_ids' ] # torch.Size([1, 77]) # \"a photograph of an astronaut riding a horse\" >>> tensor ([[ 49406 , 320 , 8853 , 539 , 550 , 18376 , 6765 , 320 , 4558 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 , 49407 ]]) tokenizer . decode ( 49407 ) >>> \"<|endoftext|>\" In addition, the text_input also has an attention mask to ignore these \"EOT\" tokens: text_input [ \"attention_mask\" ] # torch.Size([1, 77]) # \"a photograph of an astronaut riding a horse\" >>> tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]) Now one the text is run through the encoder, the appropriate embeddings are mapped over text_embeddings = text_encoder ( text_input . input_ids . to ( \"cuda\" ))[ 0 ] . half () text_embeddings . shape # >>> torch.Size([1, 77, 768]) From our discussion before, we need to generate a un-guided photo, but as a result, still need to pass empty inputs to the encoder Note that .half() means floating point 16, or fp16 or float16 which trades off decimal precision for size # ensure the input is the same length max_length = text_input . input_ids . shape [ - 1 ] # the token will be the empty string uncond_input = tokenizer ( [ \"\" ] * batch_size , padding = \"max_length\" , max_length = max_length , return_tensors = \"pt\" ) # will get the same size, but the embeddings will the the same uncond_embeddings = text_encoder ( uncond_input . input_ids . to ( \"cuda\" ))[ 0 ] . half () uncond_embeddings . shape # >>> torch.Size([1, 77, 768]) # combine both inputs together for processing purposes text_embeddings = torch . cat ([ uncond_embeddings , text_embeddings ]) normal gaussian noise is generated, and note that the noise generated is at the compressed level, not at the original size: original size: 512px x 512px x 3 compressed size: 64px x 64px x 4 # lets create some noise torch . manual_seed ( 100 ) latents = torch . randn (( batch_size , unet . in_channels , height // 8 , width // 8 )) latents = latents . to ( \"cuda\" ) . half () latents . shape \"\"\" >>> torch.Size([1, 4, 64, 64]) tensor([[[[ 0.1268, 1.3564, 0.5630, ..., -0.8042, -0.6245, -0.5884], [ 1.6699, -0.9272, -0.9761, ..., 2.3848, -0.0355, -0.3179], [ 0.3579, -1.7842, -0.3052, ..., 0.4880, -2.5781, 1.0010], ... \"\"\" # set the scheduler - currently 70 steps scheduler . set_timesteps ( num_inference_steps ) # tensor = tensor * tensor(14.6146) as an example latents = latents * scheduler . init_noise_sigma \"\"\" Normally would be a full 1000 steps, but our methods will be skipping a bit To look at the approximations. Its not really a \"timestep\" it is just means how much noise scheduler.timesteps >>> tensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304, 897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826, 796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348, ... for 70 steps ... 0.0000] scheduler.sigmas >>> tensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301, 9.6279, 8.9020, 8.2443, 7.6472, 7.1044, 6.6102, 6.1594, 5.7477, 5.3709, 5.0258, 4.7090, 4.4178, 4.1497, 3.9026, 3.6744, 3.4634, 3.2680, 3.0867, 2.9183, ... for 70 steps .... 0.000] Reminder that sigma is the 'variance' or the amount of noise, that is decreasing, starting with lots of noise and decreasing it \"\"\" from tqdm.auto import tqdm # loop through decreasing amounts of noise for i , t in enumerate ( tqdm ( scheduler . timesteps )): input = torch . cat ([ latents ] * 2 ) input = scheduler . scale_model_input ( input , t ) # predict the noise residual with torch . no_grad (): # remember the text_embeddings is [empty embeddings, prompt embeddings] # - passes some initial noise (input), # - a noise constant t, and our text ebmeddings input pred = unet ( input , t , encoder_hidden_states = text_embeddings ) . sample # pred is torch.Size([1, 4, 64, 64]) # perform guidance pred_uncond , pred_text = pred . chunk ( 2 ) # this takes the starting picture + adds some small part of the guidance pred = pred_uncond + guidance_scale * ( pred_text - pred_uncond ) # compute the \"previous\" noisy sample # the latents are updated and then will be fed in again latents = scheduler . step ( pred , t , latents ) . prev_sample \"\"\" Once this is complete, decompress the image with the VAE to get generated + adjusted photo, then feed it through PIL to take a look \"\"\" with torch . no_grad (): image = vae . decode ( 1 / 0.18215 * latents ) . sample # 0.18215 - is a constant determined by the paper # image.shape -> torch.Size([1, 3, 512, 512]) image = ( image / 2 + 0.5 ) . clamp ( 0 , 1 ) image = image [ 0 ] . detach () . cpu () . permute ( 1 , 2 , 0 ) . numpy () image = ( image * 255 ) . round () . astype ( \"uint8\" ) Image . fromarray ( image )","title":"2. Lets generate the astronaut on a horse again"},{"location":"lesson-10/A2-pipeline-under-the-hood/#3-refactoring-for-programming-agility","text":"The following code is the same as above, but will allow for reusability def text_enc ( prompts : List [ str ], maxlen : int = None ) -> torch . TensorType : \"\"\"Encodes a list of strings Returns: torch.TensorType[torch.Size([1, 77, 768]), float16] \"\"\" if maxlen is None : maxlen = tokenizer . model_max_length inp = tokenizer ( prompts , padding = \"max_length\" , max_length = maxlen , truncation = True , return_tensors = \"pt\" ) return text_encoder ( inp . input_ids . to ( \"cuda\" ))[ 0 ] . half () def mk_img ( t ): \"\"\"transforms raw decompressed image into PIL-compatible ranges + sizes\"\"\" image = ( t / 2 + 0.5 ) . clamp ( 0 , 1 ) . detach () . cpu () . permute ( 1 , 2 , 0 ) . numpy () return Image . fromarray (( image * 255 ) . round () . astype ( \"uint8\" )) def mk_samples ( prompts : List [ str ], g = 7.5 , seed = 100 , steps = 70 ): \"\"\"infers based on Unet, and returns a VAE decoded image of torch.Size([1, 3, 512, 512])\"\"\" bs = len ( prompts ) text = text_enc ( prompts ) uncond = text_enc ([ \"\" ] * bs , text . shape [ 1 ]) emb = torch . cat ([ uncond , text ]) if seed : torch . manual_seed ( seed ) latents = torch . randn (( bs , unet . in_channels , height // 8 , width // 8 )) scheduler . set_timesteps ( steps ) latents = latents . to ( \"cuda\" ) . half () * scheduler . init_noise_sigma for i , ts in enumerate ( tqdm ( scheduler . timesteps )): inp = scheduler . scale_model_input ( torch . cat ([ latents ] * 2 ), ts ) with torch . no_grad (): u , t = unet ( inp , ts , encoder_hidden_states = emb ) . sample . chunk ( 2 ) pred = u + g * ( t - u ) latents = scheduler . step ( pred , ts , latents ) . prev_sample with torch . no_grad (): return vae . decode ( 1 / 0.18215 * latents ) . sample And now can run some samples in only a few lines: prompts = [ \"a photograph of an astronaut riding a horse\" , \"an oil painting of an astronaut riding a horse in the style of grant wood\" ] # will run the exact process above images = mk_samples ( prompts )","title":"3. Refactoring for programming agility"},{"location":"lesson-10/A2-pipeline-under-the-hood/#homework","text":"try and condense Image2Image, Negative Prompts, to really understand the code","title":"Homework"},{"location":"lesson-10/A3-lists-matrices-and-more/","text":"After the Break https://github.com/fastai/course22p2/blob/master/nbs/01_matmul.ipynb Goal: get to stable diffusion from the foundations Will start with the following: base python matplotlib python standard librarys (math, os, string, pickle ...) jupyter notebooks CAVEAT : once an external library or function is created from scratch, its permitted to use the imported version in its place But I don't have a million dollars to spend on a gpu farm! This course will focus on a smaller version of the technology, and after that point, the larger public models will be used. Will aim to create our own: CLIPEncoder VAE (Autoencoder) from pathlib import Path # helps navigate files import itertools # usefull tools for working with collections + iterators import urllib # for calling websites, or downloading files import pickle , gzip # for opening + saving files, different format import math , time import os , shutil # doing file system things, copy, move, mkdir # plotting libraries import matplotlib as mpl , matplotlib.pyplot as plt Get Data - Handwritten images For demo purposes the popular MNIST dataset will be used, which are black-and-white low-resolution images of hand-written digits https://github.com/datapythonista/mnist There's also another small black + white dataset called fashion-mnist https://github.com/zalandoresearch/fashion-mnist *Other alternatives can be wget.download or requests MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true' path_data = Path ( 'data' ) path_data . mkdir ( exist_ok = True ) path_gz = path_data / 'mnist.pkl.gz' # get the file, only if it hasn't be downloaded if not path_gz . exists (): urllib . request . urlretrieve ( MNIST_URL , path_gz ) with gzip . open ( path_gz , 'rb' ) as f : # the gzip contains 4 arrays + some metadata (( x_train , y_train ), ( x_valid , y_valid ), _ ) = pickle . load ( f , encoding = 'latin-1' ) # x_train.shape, y_train.shape, x_valid.shape, y_valid.shape # ((50000, 784), (50000,), (10000, 784), (10000,)) Quick note on with , which opens a \"context\". This is a shorted way of writing the longer version file_stream = gzip . open ( path_gz , 'rb' ) pickle . load ( file_open ) file_stream . close () # the same thing with gzip . open ( path_gz , 'rb' ) as file_stream : # as long as the indentation exists, the file_stream is opern pickle . load ( file_stream ) pickle . load ( file_stream ) # ERROR, since this is outside the indentation, the filestream has been closed Lets look at the first image, + peek at some of the values, which are decimals ranging from 0 -> 1 first_image_as_list = list ( x_train [ 0 ]) first_image_as_list [ 200 : 210 ] \"\"\" [0.0, 0.0, 0.0, 0.19140625, 0.9296875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125] \"\"\" len ( first_image_as_list ) # >>> 784 because its 28 x 28 To help convert 1 long list into a list of lists. Defining a chunking function, generator, will \"yield\" only a few values at a time, and can handle un-even lengths e.g. I have 11 items, but want to see them in batches of 5 About yield and generators : https://realpython.com/introduction-to-python-generators/ def chunks ( x , sz ): for i in range ( 0 , len ( x ), sz ): yield x [ i : i + sz ] sample_vals = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 ] list ( chunks ( sample_vals , 5 )) \"\"\" [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11]] \"\"\" So applying this chunking function to the 784 vectors: # there's no color channels yet mpl . rcParams [ 'image.cmap' ] = 'gray' # print the image plt . imshow ( chunks ( first_image_as_list , 28 ) ) --image-- Another way if implementing iterator There's another helpful standard lib + function called islice which is helpful sample_vals = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 ] list ( itertools . islice ( sample_vals , 5 )) # [1, 2, 3, 4, 5] list ( itertools . islice ( sample_vals , 5 )) # [6, 7, 8, 9, 10] So what's happening with the image data, is that it will call the full list of 768 items, and will return a row of 28 each time its called. These will stack for 28 rows which will result in 28 x 28 image_iterator = iter ( first_image_as_list ) img_as_list_of_lists = list ( iter ( lambda : list ( itertools . islice ( image_iterator , 28 )), [])) plt . imshow ( img_as_list_of_lists );","title":"Lists, Matrices and More"},{"location":"lesson-10/A3-lists-matrices-and-more/#after-the-break","text":"https://github.com/fastai/course22p2/blob/master/nbs/01_matmul.ipynb Goal: get to stable diffusion from the foundations Will start with the following: base python matplotlib python standard librarys (math, os, string, pickle ...) jupyter notebooks CAVEAT : once an external library or function is created from scratch, its permitted to use the imported version in its place But I don't have a million dollars to spend on a gpu farm! This course will focus on a smaller version of the technology, and after that point, the larger public models will be used. Will aim to create our own: CLIPEncoder VAE (Autoencoder) from pathlib import Path # helps navigate files import itertools # usefull tools for working with collections + iterators import urllib # for calling websites, or downloading files import pickle , gzip # for opening + saving files, different format import math , time import os , shutil # doing file system things, copy, move, mkdir # plotting libraries import matplotlib as mpl , matplotlib.pyplot as plt","title":"After the Break"},{"location":"lesson-10/A3-lists-matrices-and-more/#get-data-handwritten-images","text":"For demo purposes the popular MNIST dataset will be used, which are black-and-white low-resolution images of hand-written digits https://github.com/datapythonista/mnist There's also another small black + white dataset called fashion-mnist https://github.com/zalandoresearch/fashion-mnist *Other alternatives can be wget.download or requests MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true' path_data = Path ( 'data' ) path_data . mkdir ( exist_ok = True ) path_gz = path_data / 'mnist.pkl.gz' # get the file, only if it hasn't be downloaded if not path_gz . exists (): urllib . request . urlretrieve ( MNIST_URL , path_gz ) with gzip . open ( path_gz , 'rb' ) as f : # the gzip contains 4 arrays + some metadata (( x_train , y_train ), ( x_valid , y_valid ), _ ) = pickle . load ( f , encoding = 'latin-1' ) # x_train.shape, y_train.shape, x_valid.shape, y_valid.shape # ((50000, 784), (50000,), (10000, 784), (10000,)) Quick note on with , which opens a \"context\". This is a shorted way of writing the longer version file_stream = gzip . open ( path_gz , 'rb' ) pickle . load ( file_open ) file_stream . close () # the same thing with gzip . open ( path_gz , 'rb' ) as file_stream : # as long as the indentation exists, the file_stream is opern pickle . load ( file_stream ) pickle . load ( file_stream ) # ERROR, since this is outside the indentation, the filestream has been closed Lets look at the first image, + peek at some of the values, which are decimals ranging from 0 -> 1 first_image_as_list = list ( x_train [ 0 ]) first_image_as_list [ 200 : 210 ] \"\"\" [0.0, 0.0, 0.0, 0.19140625, 0.9296875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125] \"\"\" len ( first_image_as_list ) # >>> 784 because its 28 x 28 To help convert 1 long list into a list of lists. Defining a chunking function, generator, will \"yield\" only a few values at a time, and can handle un-even lengths e.g. I have 11 items, but want to see them in batches of 5 About yield and generators : https://realpython.com/introduction-to-python-generators/ def chunks ( x , sz ): for i in range ( 0 , len ( x ), sz ): yield x [ i : i + sz ] sample_vals = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 ] list ( chunks ( sample_vals , 5 )) \"\"\" [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11]] \"\"\" So applying this chunking function to the 784 vectors: # there's no color channels yet mpl . rcParams [ 'image.cmap' ] = 'gray' # print the image plt . imshow ( chunks ( first_image_as_list , 28 ) ) --image--","title":"Get Data - Handwritten images"},{"location":"lesson-10/A3-lists-matrices-and-more/#another-way-if-implementing-iterator","text":"There's another helpful standard lib + function called islice which is helpful sample_vals = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 ] list ( itertools . islice ( sample_vals , 5 )) # [1, 2, 3, 4, 5] list ( itertools . islice ( sample_vals , 5 )) # [6, 7, 8, 9, 10] So what's happening with the image data, is that it will call the full list of 768 items, and will return a row of 28 each time its called. These will stack for 28 rows which will result in 28 x 28 image_iterator = iter ( first_image_as_list ) img_as_list_of_lists = list ( iter ( lambda : list ( itertools . islice ( image_iterator , 28 )), [])) plt . imshow ( img_as_list_of_lists );","title":"Another way if implementing iterator"},{"location":"lesson-10/A4-matrices/","text":"Matrices + Tensors From before: from pathlib import Path # helps navigate files import itertools # usefull tools for working with collections + iterators import urllib # for calling websites, or downloading files import pickle , gzip # for opening + saving files, different format import math , time import os , shutil # doing file system things, copy, move, mkdir # plotting libraries import matplotlib as mpl , matplotlib.pyplot as plt And loading the same handwritten dataset from before MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true' path_data = Path ( 'data' ) path_data . mkdir ( exist_ok = True ) path_gz = path_data / 'mnist.pkl.gz' # get the file, only if it hasn't be downloaded if not path_gz . exists (): urllib . request . urlretrieve ( MNIST_URL , path_gz ) with gzip . open ( path_gz , 'rb' ) as f : # the gzip contains 4 arrays + some metadata (( x_train , y_train ), ( x_valid , y_valid ), _ ) = pickle . load ( f , encoding = 'latin-1' ) # x_train.shape, y_train.shape, x_valid.shape, y_valid.shape # ((50000, 784), (50000,), (10000, 784), (10000,)) Using the first image as an example, we create a list of lists. For ease of reading, the code has been expanded to show what is being called by what first_image_as_list = list ( x_train [ 0 ]) image_iterator = iter ( first_image_as_list ) img_as_list_of_lists = list ( iter ( lambda : list ( itertools . islice ( image_iterator , 28 ) ), [] ) ) If we look at the first row, we can select a particular element # row img_as_list_of_lists [ 20 ] \"\"\" >>> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.4453125, 0.86328125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.78515625, 0.3046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \"\"\" # element img_as_list_of_lists [ 20 ][ 15 ] \"\"\" >>> 0.98828125 \"\"\" But the preferred \"usage\" or syntax is: - arr[20, 15] preferred - arr[20][15] curreent Better ways of doing matrix Let's make a class class Matrix : def __init__ ( self , xs ): self . xs = xs def __getitem__ ( self , idxs ): # note this assumes that at least 2 indices are passed return self . xs [ idxs [ 0 ]][ idxs [ 1 ]] class indicates we are defining / declaring a new class, by the name Matrix . Python documentation on classes here __<something>__ has two handlebars on both sides called double-under or dunder here's a list of some common python dunders : https://www.pythonmorsels.com/dunder-variables/ __init__ is the constructor , which means it is run everytime a new instance of the class is created self will be covered in a later section mtx = Matrix ( img_as_list_of_lists ) # this is calling the __init__ mtx [ 20 , 15 ] # calls __getitem__([20, 15]) # >>> 0.98828125 pytorch -> tensors Lets see how this is done in pytorch import torch from torch import tensor one_d = tensor ([ 1 , 2 , 3 ]) # 1D two_d = tensor ([ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], ]) # 2D matrix one_d , two_d \"\"\" (tensor([1, 2, 3]), tensor([[1, 2, 3], [4, 5, 6]])) \"\"\" img_as_tensor = tensor ( img_as_list_of_lists ) Note that all this work above was done for a single image array. Lets use python's map function to apply it to many things at once. Consider a quick example below: a , b , c , d = map ( lambda x : x + 1 , [ 1 , 2 , 3 , 4 ]) \"\"\" a, b, c, d (2, 3, 4, 5) a 2 \"\"\" The map will go through and apply our function x + 1 against each item in the list. Also notice, since we are returning 4 values, if we put exactly 4 variables on the left side, we can do individual assignments Now lets use this method on the training + validation datasets x_trn_tensor , y_trn_tensor , x_val_tensor , y_val_tensor = map ( tensor , ( x_train , y_train , x_valid , y_valid )) x_trn_tensor . shape # >>> torch.Size([50000, 784]) properties of torch.tensor .shape : tells the size of the different dimensions .type() : tells the type, float , long , int and will also tell the precision 16, 32, 64 .reshape(shape=new_size) : can re-arrange the elements into a different shape t1 = torch . tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]) t2 = t1 . reshape ( shape = ( 2 , 6 )) t3 = t1 . reshape ( shape = ( 3 , 4 )) t4 = t1 . reshape ( shape = ( 12 , 1 )) t1 . shape , t2 . shape , t3 . shape , t4 . shape \"\"\" (torch.Size([12]), torch.Size([2, 6]), torch.Size([3, 4]), torch.Size([12, 1])) \"\"\" t2 \"\"\" tensor([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12]]) \"\"\" Caveat : pay close attention to 1-Dish tensors: shape=(12,) is not the same as shape=(12, 1) or shape=(1, 12) # note the original was [50000, 784] imgs = x_trn_tensor . reshape (( - 1 , 28 , 28 )) imgs . shape \"\"\" torch.Size([50000, 28, 28]) \"\"\" plt . imshow ( imgs [ 0 ]) Note that the -1 means \"keep the first dimension\" --image-- A quick word on vocab APL is a programming language developed closer to the expressions found in math. (https://tryapl.org/)[https://tryapl.org/]. In APL, they don't use the word tensor they use the word arrays . numpy which was heavily influenced by APL also borrowed the language and called these arrays . Pytorch which was heavily influenced by numpy, for some reason calls them tensors . 1-D tensor : is like a vector, or list 2-D tensor : is like a matrix, or spreadsheet 3-D tensor : is like a cube, a batch of matrices, or a stack of spreadsheets and can be much higher order dimensions! Fast.ai has a APL study Forum Language of Tensors Rank - how many dimensions are there? Here's an example of a rank-1 tensor z = torch . tensor ([ 1 , 2 , 3 , 4 ]) z . shape # torch.Size([4]) Note the extra nested list z = torch . tensor ([[ 1 , 2 , 3 , 4 ],]) z . shape # torch.Size([1, 4]) Now considering all our images, will be rank-3 imgs . shape # torch.Size([50000, 28, 28]) A single image would be a rank-2 matrix imgs [ 0 ] . shape # torch.Size([28, 28]) Extract information about the dataset from the tensors n_records , n_pixels = x_trn_tensor . shape # how many targets min ( y_trn_tensor ) # 9 max ( y_trn_tensor ) # 0 So there are a total of 10 classes, and they will be labels","title":"More Matrix Operations"},{"location":"lesson-10/A4-matrices/#matrices-tensors","text":"From before: from pathlib import Path # helps navigate files import itertools # usefull tools for working with collections + iterators import urllib # for calling websites, or downloading files import pickle , gzip # for opening + saving files, different format import math , time import os , shutil # doing file system things, copy, move, mkdir # plotting libraries import matplotlib as mpl , matplotlib.pyplot as plt And loading the same handwritten dataset from before MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true' path_data = Path ( 'data' ) path_data . mkdir ( exist_ok = True ) path_gz = path_data / 'mnist.pkl.gz' # get the file, only if it hasn't be downloaded if not path_gz . exists (): urllib . request . urlretrieve ( MNIST_URL , path_gz ) with gzip . open ( path_gz , 'rb' ) as f : # the gzip contains 4 arrays + some metadata (( x_train , y_train ), ( x_valid , y_valid ), _ ) = pickle . load ( f , encoding = 'latin-1' ) # x_train.shape, y_train.shape, x_valid.shape, y_valid.shape # ((50000, 784), (50000,), (10000, 784), (10000,)) Using the first image as an example, we create a list of lists. For ease of reading, the code has been expanded to show what is being called by what first_image_as_list = list ( x_train [ 0 ]) image_iterator = iter ( first_image_as_list ) img_as_list_of_lists = list ( iter ( lambda : list ( itertools . islice ( image_iterator , 28 ) ), [] ) ) If we look at the first row, we can select a particular element # row img_as_list_of_lists [ 20 ] \"\"\" >>> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.4453125, 0.86328125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.78515625, 0.3046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \"\"\" # element img_as_list_of_lists [ 20 ][ 15 ] \"\"\" >>> 0.98828125 \"\"\" But the preferred \"usage\" or syntax is: - arr[20, 15] preferred - arr[20][15] curreent","title":"Matrices + Tensors"},{"location":"lesson-10/A4-matrices/#better-ways-of-doing-matrix","text":"Let's make a class class Matrix : def __init__ ( self , xs ): self . xs = xs def __getitem__ ( self , idxs ): # note this assumes that at least 2 indices are passed return self . xs [ idxs [ 0 ]][ idxs [ 1 ]] class indicates we are defining / declaring a new class, by the name Matrix . Python documentation on classes here __<something>__ has two handlebars on both sides called double-under or dunder here's a list of some common python dunders : https://www.pythonmorsels.com/dunder-variables/ __init__ is the constructor , which means it is run everytime a new instance of the class is created self will be covered in a later section mtx = Matrix ( img_as_list_of_lists ) # this is calling the __init__ mtx [ 20 , 15 ] # calls __getitem__([20, 15]) # >>> 0.98828125","title":"Better ways of doing matrix"},{"location":"lesson-10/A4-matrices/#pytorch-tensors","text":"Lets see how this is done in pytorch import torch from torch import tensor one_d = tensor ([ 1 , 2 , 3 ]) # 1D two_d = tensor ([ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ], ]) # 2D matrix one_d , two_d \"\"\" (tensor([1, 2, 3]), tensor([[1, 2, 3], [4, 5, 6]])) \"\"\" img_as_tensor = tensor ( img_as_list_of_lists ) Note that all this work above was done for a single image array. Lets use python's map function to apply it to many things at once. Consider a quick example below: a , b , c , d = map ( lambda x : x + 1 , [ 1 , 2 , 3 , 4 ]) \"\"\" a, b, c, d (2, 3, 4, 5) a 2 \"\"\" The map will go through and apply our function x + 1 against each item in the list. Also notice, since we are returning 4 values, if we put exactly 4 variables on the left side, we can do individual assignments Now lets use this method on the training + validation datasets x_trn_tensor , y_trn_tensor , x_val_tensor , y_val_tensor = map ( tensor , ( x_train , y_train , x_valid , y_valid )) x_trn_tensor . shape # >>> torch.Size([50000, 784]) properties of torch.tensor .shape : tells the size of the different dimensions .type() : tells the type, float , long , int and will also tell the precision 16, 32, 64 .reshape(shape=new_size) : can re-arrange the elements into a different shape t1 = torch . tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]) t2 = t1 . reshape ( shape = ( 2 , 6 )) t3 = t1 . reshape ( shape = ( 3 , 4 )) t4 = t1 . reshape ( shape = ( 12 , 1 )) t1 . shape , t2 . shape , t3 . shape , t4 . shape \"\"\" (torch.Size([12]), torch.Size([2, 6]), torch.Size([3, 4]), torch.Size([12, 1])) \"\"\" t2 \"\"\" tensor([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12]]) \"\"\" Caveat : pay close attention to 1-Dish tensors: shape=(12,) is not the same as shape=(12, 1) or shape=(1, 12) # note the original was [50000, 784] imgs = x_trn_tensor . reshape (( - 1 , 28 , 28 )) imgs . shape \"\"\" torch.Size([50000, 28, 28]) \"\"\" plt . imshow ( imgs [ 0 ]) Note that the -1 means \"keep the first dimension\" --image--","title":"pytorch -&gt; tensors"},{"location":"lesson-10/A4-matrices/#a-quick-word-on-vocab","text":"APL is a programming language developed closer to the expressions found in math. (https://tryapl.org/)[https://tryapl.org/]. In APL, they don't use the word tensor they use the word arrays . numpy which was heavily influenced by APL also borrowed the language and called these arrays . Pytorch which was heavily influenced by numpy, for some reason calls them tensors . 1-D tensor : is like a vector, or list 2-D tensor : is like a matrix, or spreadsheet 3-D tensor : is like a cube, a batch of matrices, or a stack of spreadsheets and can be much higher order dimensions! Fast.ai has a APL study Forum","title":"A quick word on vocab"},{"location":"lesson-10/A4-matrices/#language-of-tensors","text":"","title":"Language of Tensors"},{"location":"lesson-10/A4-matrices/#rank-how-many-dimensions-are-there","text":"Here's an example of a rank-1 tensor z = torch . tensor ([ 1 , 2 , 3 , 4 ]) z . shape # torch.Size([4]) Note the extra nested list z = torch . tensor ([[ 1 , 2 , 3 , 4 ],]) z . shape # torch.Size([1, 4]) Now considering all our images, will be rank-3 imgs . shape # torch.Size([50000, 28, 28]) A single image would be a rank-2 matrix imgs [ 0 ] . shape # torch.Size([28, 28])","title":"Rank - how many dimensions are there?"},{"location":"lesson-10/A4-matrices/#extract-information-about-the-dataset-from-the-tensors","text":"n_records , n_pixels = x_trn_tensor . shape # how many targets min ( y_trn_tensor ) # 9 max ( y_trn_tensor ) # 0 So there are a total of 10 classes, and they will be labels","title":"Extract information about the dataset from the tensors"},{"location":"lesson-10/A5-random_numbers/","text":"Random Numbers Building a generator from scratch. There is no way to make a random number. Computers can track time, add + subtract numbers. Most programs will generate a number that LOOKS random. Generally: rand() should be different from call to call rand() should be uniformly distributed (all numbers have an equal chance of being selected) Wichmann - Hill: algorithm random_state = None def seed ( a ): global random_state a , x = divmod ( a , 30268 ) a , y = divmod ( a , 30306 ) a , z = divmod ( a , 30322 ) # store these 3 vals in random state random_state = int ( x ) + 1 , int ( y ) + 1 , int ( z ) + 1 def rand (): global random_state x , y , z = random_state x = ( 171 * x ) % 30269 y = ( 172 * y ) % 30307 z = ( 170 * z ) % 30323 random_state = x , y , z return ( x / 30269 + y / 30307 + z / 30323 ) % 1.0 # pulls out the decimal part Number generates rely on this state . rand() will keep generating random numbers as long as the random state is passed to the next function if os . fork (): print ( f \"in parent { rand () } \" ) else : print ( f \"In child: { rand () } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent 0.030043691943175688 In child: 0.030043691943175688 \"\"\" Oops. they are the same, this is because during the fork the random_state was also copied along with copying the processes, meaning that the 2nd execution will result in the same generated number. This might be intentional, but in other cases it might be the opposite of what is desired: using parallel processes to augment images differently (should not repeat!) to do this the random number generator must be initialized in each process individually! even pytorch out of the box has this same issue torch import torch if os . fork (): print ( f \"in parent { torch . rand ( 1 ) } \" ) else : print ( f \"In child: { torch . rand ( 1 ) } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent tensor([0.3805]) In child: tensor([0.3805]) Also the same! \"\"\" numpy import numpy as np if os . fork (): print ( f \"in parent { np . random . rand ( 1 ) } \" ) else : print ( f \"In child: { np . random . rand ( 1 ) } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent tensor([0.3805]) In child: tensor([0.3805]) Also the same! \"\"\" base python from random import random if os . fork (): print ( f \"in parent { random ( 1 ) } \" ) else : print ( f \"In child: { random ( 1 ) } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent 0.9554070280183096 In child: 0.7616488033957549 Not the same \"\"\" What about speed Our method # from before %% timeit - n 10 def chunks ( x , sz ): for i in range ( 0 , len ( x ), sz ): yield x [ i : i + sz ] list ( chunks ([ rand () for _ in range ( 7840 )], 10 )) # 4.46 ms \u00b1 66.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) The torch method %% timeit - n 10 torch . randn ( 784 , 10 ) \"\"\" The slowest run took 4.16 times longer than the fastest. This could mean that an intermediate result is being cached. 61.6 \u00b5s \u00b1 45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) \"\"\"","title":"Random Numbers"},{"location":"lesson-10/A5-random_numbers/#random-numbers","text":"Building a generator from scratch. There is no way to make a random number. Computers can track time, add + subtract numbers. Most programs will generate a number that LOOKS random. Generally: rand() should be different from call to call rand() should be uniformly distributed (all numbers have an equal chance of being selected) Wichmann - Hill: algorithm random_state = None def seed ( a ): global random_state a , x = divmod ( a , 30268 ) a , y = divmod ( a , 30306 ) a , z = divmod ( a , 30322 ) # store these 3 vals in random state random_state = int ( x ) + 1 , int ( y ) + 1 , int ( z ) + 1 def rand (): global random_state x , y , z = random_state x = ( 171 * x ) % 30269 y = ( 172 * y ) % 30307 z = ( 170 * z ) % 30323 random_state = x , y , z return ( x / 30269 + y / 30307 + z / 30323 ) % 1.0 # pulls out the decimal part Number generates rely on this state . rand() will keep generating random numbers as long as the random state is passed to the next function if os . fork (): print ( f \"in parent { rand () } \" ) else : print ( f \"In child: { rand () } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent 0.030043691943175688 In child: 0.030043691943175688 \"\"\" Oops. they are the same, this is because during the fork the random_state was also copied along with copying the processes, meaning that the 2nd execution will result in the same generated number. This might be intentional, but in other cases it might be the opposite of what is desired: using parallel processes to augment images differently (should not repeat!) to do this the random number generator must be initialized in each process individually! even pytorch out of the box has this same issue","title":"Random Numbers"},{"location":"lesson-10/A5-random_numbers/#torch","text":"import torch if os . fork (): print ( f \"in parent { torch . rand ( 1 ) } \" ) else : print ( f \"In child: { torch . rand ( 1 ) } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent tensor([0.3805]) In child: tensor([0.3805]) Also the same! \"\"\"","title":"torch"},{"location":"lesson-10/A5-random_numbers/#numpy","text":"import numpy as np if os . fork (): print ( f \"in parent { np . random . rand ( 1 ) } \" ) else : print ( f \"In child: { np . random . rand ( 1 ) } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent tensor([0.3805]) In child: tensor([0.3805]) Also the same! \"\"\"","title":"numpy"},{"location":"lesson-10/A5-random_numbers/#base-python","text":"from random import random if os . fork (): print ( f \"in parent { random ( 1 ) } \" ) else : print ( f \"In child: { random ( 1 ) } \" ) os . _exit ( os . EX_OK ) \"\"\" in parent 0.9554070280183096 In child: 0.7616488033957549 Not the same \"\"\"","title":"base python"},{"location":"lesson-10/A5-random_numbers/#what-about-speed","text":"Our method # from before %% timeit - n 10 def chunks ( x , sz ): for i in range ( 0 , len ( x ), sz ): yield x [ i : i + sz ] list ( chunks ([ rand () for _ in range ( 7840 )], 10 )) # 4.46 ms \u00b1 66.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) The torch method %% timeit - n 10 torch . randn ( 784 , 10 ) \"\"\" The slowest run took 4.16 times longer than the fastest. This could mean that an intermediate result is being cached. 61.6 \u00b5s \u00b1 45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) \"\"\"","title":"What about speed"},{"location":"lesson-10b/A1-diffedit/","text":"","title":"A1 diffedit"},{"location":"lesson-11/A1-diffedit-paper/","text":"Reading a Paper together: Diffedit DiffEdit: Diffusion-based semantic image editing with mask guidance Arxiv link Zotero - chrome menu bar. The paper will appear in the software ( zotero ) can have the url has metadata can also read the paper annotate, edit + organize to papers Observations First sentence == \"wow, how great is current advances!\" Second sentence = \"here is DiffEdit\" Third sentence = \"when an image and prompt are given, the generated image should retain as much of the original as possible\" Then from the pictures, it looks like the text prompt is closely aligned to the original image, so the generated image should only change what is requested. Main contribution Previous techniques usually require a \"mask\" to be manually supplied, but this papers' main contribution is to dynamically find the mask itself Introductions to papers Talks about what its trying to do, tries to describe the problem + provide an overview to current methods. Looking at related work is a good place to look into current papers. Background Scary time. A lot of equations. Background is often written last and intended to look smart to the reviewers. Just a note: No one in the world is going to look at this paragraph and immediately know. Let's walk through the math part of the papers. Super helpful tip: learn the greek alphabet to interpret the equations Equation 1 can be read as follows: L = \"loss\" epsilon = \"true noise\" epsilon_theta = \"noise estimator\" - X_t = is the image at step `t` - t = the step number || epsilon - epsilon_theta(X_t, t)|| is the rank 2 norm What does double pipe mean: Quora link What does the super + sub script mean: Quora link means the root sum of squares What does the capital E mean? Expected value operator. In statistics the expected value is often the weighted average Say you have 50% chance of willing $10, the E(situation) = 0.5 x $10 = $5 which means \"in-general\" people will win $5 Looking at the \"picture\" algorithm diagram With walking through the talk step by step, the big idea is: run inference on the original image for the truth horse vs. zebra and then do a diff. for the zebra inference, it will highlight the pixels relating to the animal and then leave the background alone after inferring on both, doing the diff, will show that the background is the same on both images, this will be our MASK then when going through the normal diffusion process, at every step, will replace the MASKED area (background) with the original, to ensure that the pixels remain the same Comment on Appendices: often contain experiments or lessons learned while developing the process HW comments using hugging face's pipeline and the provided code, try and create the above paper's results!","title":"DiffEdit Paper"},{"location":"lesson-11/A1-diffedit-paper/#reading-a-paper-together-diffedit","text":"DiffEdit: Diffusion-based semantic image editing with mask guidance Arxiv link Zotero - chrome menu bar. The paper will appear in the software ( zotero ) can have the url has metadata can also read the paper annotate, edit + organize to papers","title":"Reading a Paper together: Diffedit"},{"location":"lesson-11/A1-diffedit-paper/#observations","text":"First sentence == \"wow, how great is current advances!\" Second sentence = \"here is DiffEdit\" Third sentence = \"when an image and prompt are given, the generated image should retain as much of the original as possible\" Then from the pictures, it looks like the text prompt is closely aligned to the original image, so the generated image should only change what is requested. Main contribution Previous techniques usually require a \"mask\" to be manually supplied, but this papers' main contribution is to dynamically find the mask itself","title":"Observations"},{"location":"lesson-11/A1-diffedit-paper/#introductions-to-papers","text":"Talks about what its trying to do, tries to describe the problem + provide an overview to current methods. Looking at related work is a good place to look into current papers.","title":"Introductions to papers"},{"location":"lesson-11/A1-diffedit-paper/#background","text":"Scary time. A lot of equations. Background is often written last and intended to look smart to the reviewers. Just a note: No one in the world is going to look at this paragraph and immediately know. Let's walk through the math part of the papers. Super helpful tip: learn the greek alphabet to interpret the equations Equation 1 can be read as follows: L = \"loss\" epsilon = \"true noise\" epsilon_theta = \"noise estimator\" - X_t = is the image at step `t` - t = the step number || epsilon - epsilon_theta(X_t, t)|| is the rank 2 norm What does double pipe mean: Quora link What does the super + sub script mean: Quora link means the root sum of squares What does the capital E mean? Expected value operator. In statistics the expected value is often the weighted average Say you have 50% chance of willing $10, the E(situation) = 0.5 x $10 = $5 which means \"in-general\" people will win $5","title":"Background"},{"location":"lesson-11/A1-diffedit-paper/#looking-at-the-picture-algorithm-diagram","text":"With walking through the talk step by step, the big idea is: run inference on the original image for the truth horse vs. zebra and then do a diff. for the zebra inference, it will highlight the pixels relating to the animal and then leave the background alone after inferring on both, doing the diff, will show that the background is the same on both images, this will be our MASK then when going through the normal diffusion process, at every step, will replace the MASKED area (background) with the original, to ensure that the pixels remain the same","title":"Looking at the \"picture\" algorithm diagram"},{"location":"lesson-11/A1-diffedit-paper/#comment-on-appendices","text":"often contain experiments or lessons learned while developing the process","title":"Comment on Appendices:"},{"location":"lesson-11/A1-diffedit-paper/#hw-comments","text":"using hugging face's pipeline and the provided code, try and create the above paper's results!","title":"HW comments"},{"location":"lesson-11/A2-matrix_operations/","text":"Matrix multiplication from foundations from pathlib import Path # helps navigate files import itertools # usefull tools for working with collections + iterators import urllib # for calling websites, or downloading files import pickle , gzip # for opening + saving files, different format import math , time import os , shutil # doing file system things, copy, move, mkdir # plotting libraries import matplotlib as mpl , matplotlib.pyplot as plt import torch from torch import tensor # our tensor + deeplearning framework # set some printing options torch . set_printoptions ( precision = 2 , linewidth = 140 , sci_mode = False ) MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true' path_data = Path ( 'data' ) path_data . mkdir ( exist_ok = True ) path_gz = path_data / 'mnist.pkl.gz' # get the file, only if it hasn't be downloaded if not path_gz . exists (): urllib . request . urlretrieve ( MNIST_URL , path_gz ) with gzip . open ( path_gz , 'rb' ) as f : # the gzip contains 4 arrays + some metadata (( x_train , y_train ), ( x_valid , y_valid ), _ ) = pickle . load ( f , encoding = 'latin-1' ) # x_train.shape, y_train.shape, x_valid.shape, y_valid.shape # ((50000, 784), (50000,), (10000, 784), (10000,)) # convert all downloaded data into pytorch tensors x_trn_tensor , y_trn_tensor , x_val_tensor , y_val_tensor = map ( tensor , ( x_train , y_train , x_valid , y_valid )) # reshape the training input into 28 x 28 images imgs = x_trn_tensor . reshape (( - 1 , 28 , 28 )) 1. A preview of setting up linear weight multiplication torch . manual_seed ( 1 ) weights = torch . randn ( 784 , 10 ) bias = torch . zeros ( 10 ) since we will be doing a small mini-batch for illustration purposes A = x_valid [: 5 ] # 5 x 784 B = weights # 784 x 10 1.1 General rule when multiplying matrices The inner dimension must match the output will collapse the common dimension A_rows , A_cols = A . shape B_rows , B_cols = B . shape # will be our output holder output = torch . zeros ( A_rows , B_cols ) Lets write the double loop for this (inefficient i know!) for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] output \"\"\" tensor([[-10.94, -0.68, -7.00, -4.01, -2.09, -3.36, 3.91, -3.44, -11.47, -2.12], [ 14.54, 6.00, 2.89, -4.08, 6.59, -14.74, -9.28, 2.16, -15.28, -2.68], [ 2.22, -3.22, -4.80, -6.05, 14.17, -8.98, -4.79, -5.44, -20.68, 13.57], [ -6.71, 8.90, -7.46, -7.90, 2.70, -4.73, -11.03, -12.98, -6.44, 3.64], [ -2.44, -6.40, -2.40, -9.04, 11.18, -5.77, -8.92, -3.79, -8.98, 5.28]]) \"\"\" 1.2 Wrapping all into a function: def matmul ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] return output Do a quick timing test %%timeit matmul(A, B) # 741 ms \u00b1 1.64 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Egad that is slow, lets try and speed it up 1.3 Numba numba is a system that takes python and turns it into machine code from numba import njit from numpy import array An example @njit def dot ( a , b ): res = 0. for i in range ( len ( a )): res += a [ i ] * b [ i ] return res Time it twice, the first time will compile it, the 2nd time will let it run %% timeit dot ( array ([ 1. , 2. , 3. ]), array ([ 4. , 5. , 6. ]) ) # 5.11 \u00b5s \u00b1 7.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) For the second test, runs much faster! %% timeit dot ( array ([ 1. , 2. , 3. ]), array ([ 4. , 5. , 6. ]) ) # 1.85 \u00b5s \u00b1 8.05 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each) Now if we update the function: def matmul_numba ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the numba-powered array calculation output [ i , j ] = dot ( A [ i , :], B [:, j ]) return output and now time the process %% timeit matmul ( A , B ) # 846 ms \u00b1 27.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_numba ( A . numpy (), B . numpy ()) # 340 \u00b5s \u00b1 888 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) Note the units! its a 2000x speed up","title":"Matrix Operations"},{"location":"lesson-11/A2-matrix_operations/#matrix-multiplication-from-foundations","text":"from pathlib import Path # helps navigate files import itertools # usefull tools for working with collections + iterators import urllib # for calling websites, or downloading files import pickle , gzip # for opening + saving files, different format import math , time import os , shutil # doing file system things, copy, move, mkdir # plotting libraries import matplotlib as mpl , matplotlib.pyplot as plt import torch from torch import tensor # our tensor + deeplearning framework # set some printing options torch . set_printoptions ( precision = 2 , linewidth = 140 , sci_mode = False ) MNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true' path_data = Path ( 'data' ) path_data . mkdir ( exist_ok = True ) path_gz = path_data / 'mnist.pkl.gz' # get the file, only if it hasn't be downloaded if not path_gz . exists (): urllib . request . urlretrieve ( MNIST_URL , path_gz ) with gzip . open ( path_gz , 'rb' ) as f : # the gzip contains 4 arrays + some metadata (( x_train , y_train ), ( x_valid , y_valid ), _ ) = pickle . load ( f , encoding = 'latin-1' ) # x_train.shape, y_train.shape, x_valid.shape, y_valid.shape # ((50000, 784), (50000,), (10000, 784), (10000,)) # convert all downloaded data into pytorch tensors x_trn_tensor , y_trn_tensor , x_val_tensor , y_val_tensor = map ( tensor , ( x_train , y_train , x_valid , y_valid )) # reshape the training input into 28 x 28 images imgs = x_trn_tensor . reshape (( - 1 , 28 , 28 ))","title":"Matrix multiplication from foundations"},{"location":"lesson-11/A2-matrix_operations/#1-a-preview-of-setting-up-linear-weight-multiplication","text":"torch . manual_seed ( 1 ) weights = torch . randn ( 784 , 10 ) bias = torch . zeros ( 10 ) since we will be doing a small mini-batch for illustration purposes A = x_valid [: 5 ] # 5 x 784 B = weights # 784 x 10","title":"1. A preview of setting up linear weight multiplication"},{"location":"lesson-11/A2-matrix_operations/#11-general-rule-when-multiplying-matrices","text":"The inner dimension must match the output will collapse the common dimension A_rows , A_cols = A . shape B_rows , B_cols = B . shape # will be our output holder output = torch . zeros ( A_rows , B_cols ) Lets write the double loop for this (inefficient i know!) for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] output \"\"\" tensor([[-10.94, -0.68, -7.00, -4.01, -2.09, -3.36, 3.91, -3.44, -11.47, -2.12], [ 14.54, 6.00, 2.89, -4.08, 6.59, -14.74, -9.28, 2.16, -15.28, -2.68], [ 2.22, -3.22, -4.80, -6.05, 14.17, -8.98, -4.79, -5.44, -20.68, 13.57], [ -6.71, 8.90, -7.46, -7.90, 2.70, -4.73, -11.03, -12.98, -6.44, 3.64], [ -2.44, -6.40, -2.40, -9.04, 11.18, -5.77, -8.92, -3.79, -8.98, 5.28]]) \"\"\"","title":"1.1 General rule when multiplying matrices"},{"location":"lesson-11/A2-matrix_operations/#12-wrapping-all-into-a-function","text":"def matmul ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] return output Do a quick timing test %%timeit matmul(A, B) # 741 ms \u00b1 1.64 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Egad that is slow, lets try and speed it up","title":"1.2 Wrapping all into a function:"},{"location":"lesson-11/A2-matrix_operations/#13-numba","text":"numba is a system that takes python and turns it into machine code from numba import njit from numpy import array An example @njit def dot ( a , b ): res = 0. for i in range ( len ( a )): res += a [ i ] * b [ i ] return res Time it twice, the first time will compile it, the 2nd time will let it run %% timeit dot ( array ([ 1. , 2. , 3. ]), array ([ 4. , 5. , 6. ]) ) # 5.11 \u00b5s \u00b1 7.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) For the second test, runs much faster! %% timeit dot ( array ([ 1. , 2. , 3. ]), array ([ 4. , 5. , 6. ]) ) # 1.85 \u00b5s \u00b1 8.05 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each) Now if we update the function: def matmul_numba ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the numba-powered array calculation output [ i , j ] = dot ( A [ i , :], B [:, j ]) return output and now time the process %% timeit matmul ( A , B ) # 846 ms \u00b1 27.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_numba ( A . numpy (), B . numpy ()) # 340 \u00b5s \u00b1 888 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) Note the units! its a 2000x speed up","title":"1.3 Numba"},{"location":"lesson-11/A3-elementwise-ops/","text":"import torch 1.4 Elementwise operations a = torch . tensor ([ 10 , 6 , 4 ]) b = torch . tensor ([ 2 , 8 , 7 ]) Torch has a lot of operations built in Addition a + b # tensor([12, 14, 11]) Comparison a > b # tensor([ True, False, False]) A note about these boolean expressions: they are stored as integers: 1, 0 == True, False . As a result, can be summed, averaged etc. (a > b).sum() # tensor(1) (a > b).float().mean() # tensor(0.33) Matrix Norm: Make a 3 x 3 Matrix mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) \"\"\" tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \"\"\" Frobenius norm: for each element, square the value, add up all squares then square root the last total ( mtx * mtx ) . sum () . sqrt () # tensor(16.88) 1.5 Navigating a matrix mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) \"\"\" tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \"\"\" View a row mtx [ 1 , :] # tensor([4, 5, 6]) View a column mtx [:, 1 ] tensor ([ 2 , 5 , 8 ]) Note that if only one dimension is provided: mtx [ 1 ] # tensor([4, 5, 6]) 1.6 Using tensor operations in Matmul def matmul_tensor ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the tensor-powered array calculation output [ i , j ] = ( A [ i , :] * B [:, j ]) . sum () return output # optional adjustment def matmul_tensor ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the tensor-powered array calculation output [ i , j ] = torch . dot ( A [ i , :], B [:, j ]) return output %% timeit matmul ( A , B ) # 846 ms \u00b1 27.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_numba ( A . numpy (), B . numpy ()) # 340 \u00b5s \u00b1 888 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) %% timeit matmul_tensor ( A , B ) # 731 \u00b5s \u00b1 2.88 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)","title":"Elementwise Operations"},{"location":"lesson-11/A3-elementwise-ops/#14-elementwise-operations","text":"a = torch . tensor ([ 10 , 6 , 4 ]) b = torch . tensor ([ 2 , 8 , 7 ]) Torch has a lot of operations built in","title":"1.4 Elementwise operations"},{"location":"lesson-11/A3-elementwise-ops/#addition","text":"a + b # tensor([12, 14, 11])","title":"Addition"},{"location":"lesson-11/A3-elementwise-ops/#comparison","text":"a > b # tensor([ True, False, False]) A note about these boolean expressions: they are stored as integers: 1, 0 == True, False . As a result, can be summed, averaged etc. (a > b).sum() # tensor(1) (a > b).float().mean() # tensor(0.33)","title":"Comparison"},{"location":"lesson-11/A3-elementwise-ops/#matrix-norm","text":"Make a 3 x 3 Matrix mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) \"\"\" tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \"\"\" Frobenius norm: for each element, square the value, add up all squares then square root the last total ( mtx * mtx ) . sum () . sqrt () # tensor(16.88)","title":"Matrix Norm:"},{"location":"lesson-11/A3-elementwise-ops/#15-navigating-a-matrix","text":"mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) \"\"\" tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \"\"\" View a row mtx [ 1 , :] # tensor([4, 5, 6]) View a column mtx [:, 1 ] tensor ([ 2 , 5 , 8 ]) Note that if only one dimension is provided: mtx [ 1 ] # tensor([4, 5, 6])","title":"1.5 Navigating a matrix"},{"location":"lesson-11/A3-elementwise-ops/#16-using-tensor-operations-in-matmul","text":"def matmul_tensor ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the tensor-powered array calculation output [ i , j ] = ( A [ i , :] * B [:, j ]) . sum () return output # optional adjustment def matmul_tensor ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the tensor-powered array calculation output [ i , j ] = torch . dot ( A [ i , :], B [:, j ]) return output %% timeit matmul ( A , B ) # 846 ms \u00b1 27.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_numba ( A . numpy (), B . numpy ()) # 340 \u00b5s \u00b1 888 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) %% timeit matmul_tensor ( A , B ) # 731 \u00b5s \u00b1 2.88 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)","title":"1.6 Using tensor operations in Matmul"},{"location":"lesson-11/A4-broadcasting/","text":"Broadcasting The term broadcasting what if you have arrays with different shapes (rows / columns)? From the numpy documentation : The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is \u201cbroadcast\u201d across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. Broadcasting with a Scalars Even though there is only 1 value, it is broadcasted across all the elements mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) \"\"\" tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \"\"\" Addition mtx + 1 \"\"\" tensor([[ 2, 3, 4], [ 5, 6, 7], [ 8, 9, 10]]) \"\"\" Comparison mtx > 5 \"\"\" tensor([[False, False, False], [False, False, True], [ True, True, True]]) \"\"\" Broadcasting Vectors onto a matrix Say we want to multiply vec to every row in a matrix vec = torch . tensor ([ 10. , 20. , 30. ]) vec . shape , mtx . shape # (torch.Size([3]), torch.Size([3, 3])) and even though the shapes are different, watch as they are added together mtx + vec \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" vec + mtx \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" Whats going on underneath? a function called .expand_as() , and though it LOOKs like its copying the data, it is not. t = vec . expand_as ( mtx ) \"\"\" tensor([[10., 20., 30.], [10., 20., 30.], [10., 20., 30.]]) \"\"\" Digging into the intermediate tensor: its the same values with a stride for \"walking\" t . storage () \"\"\" 10.0 20.0 30.0 [torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 3] \"\"\" t . stride () # (0, 1) About 1D Vectors + expanding dimensions There is a distinction between the following: - tensor size(3) - tensor size(1, 3) - tensor size(3, 1) - This distinction will be essential to master : matrix to matrix operations vec = torch . tensor ([ 10. , 20. , 30. ]) vec . shape \"\"\" torch.Size([3]) \"\"\" How to convert a vector into a single row matrix vec_adj = vec . unsqueeze ( 0 ) vec_adj . shape \"\"\" torch.Size([1, 3]) \"\"\" vec_adj = vec [ None , :] vec_adj . shape \"\"\" torch.Size([1, 3]) \"\"\" vec_adj \"\"\" tensor([[10., 20., 30.]]) \"\"\" How to convert a vector into a single column matrix vec_adj = vec . unsqueeze ( 1 ) vec_adj . shape \"\"\" torch.Size([3, 1]) \"\"\" vec_adj = vec [:, None ] vec_adj . shape \"\"\" torch.Size([3, 1]) \"\"\" vec_adj \"\"\" tensor([[10.], [20.], [30.]]) \"\"\" Note: trailing dimensions torch has a specific syntax to skip dimensions with ... A quick example: some_tensor = torch . randn ( 2 , 3 , 4 ) some_tensor . shape # torch.Size([2, 3, 4]) some_tensor [ ... , None ] . shape # torch.Size([2, 3, 4, 1]) some_tensor [ None , ... ] . shape # torch.Size([1, 2, 3, 4]) How to control the broadcasting dimension since we combining a size(3) and a size(3, 3) , there's a question of if its doing it row-wise or column wise. This can be controlled by adjusting the vector ahead of time. vec = torch . tensor ([ 10. , 20. , 30. ]) mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) Lets look at the default: vec + mtx \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" Now lets expand the dimensions of the vector vec [ None , ... ] + mtx # (1, 3) \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" vec [ ... , None ] + mtx # (3, 1) \"\"\" tensor([[11., 12., 13.], [24., 25., 26.], [37., 38., 39.]]) \"\"\" Expanding the first dimension, applies the vector row wise Expanding the last dimension applies the vector column wise Other Broadcasing Examples vec [ None , :] # torch.Size([1, 3]) vec [:, None ] #","title":"Broadcasting"},{"location":"lesson-11/A4-broadcasting/#broadcasting","text":"The term broadcasting what if you have arrays with different shapes (rows / columns)? From the numpy documentation : The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is \u201cbroadcast\u201d across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations.","title":"Broadcasting"},{"location":"lesson-11/A4-broadcasting/#broadcasting-with-a-scalars","text":"Even though there is only 1 value, it is broadcasted across all the elements mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) \"\"\" tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \"\"\" Addition mtx + 1 \"\"\" tensor([[ 2, 3, 4], [ 5, 6, 7], [ 8, 9, 10]]) \"\"\" Comparison mtx > 5 \"\"\" tensor([[False, False, False], [False, False, True], [ True, True, True]]) \"\"\"","title":"Broadcasting with a Scalars"},{"location":"lesson-11/A4-broadcasting/#broadcasting-vectors-onto-a-matrix","text":"Say we want to multiply vec to every row in a matrix vec = torch . tensor ([ 10. , 20. , 30. ]) vec . shape , mtx . shape # (torch.Size([3]), torch.Size([3, 3])) and even though the shapes are different, watch as they are added together mtx + vec \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" vec + mtx \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" Whats going on underneath? a function called .expand_as() , and though it LOOKs like its copying the data, it is not. t = vec . expand_as ( mtx ) \"\"\" tensor([[10., 20., 30.], [10., 20., 30.], [10., 20., 30.]]) \"\"\" Digging into the intermediate tensor: its the same values with a stride for \"walking\" t . storage () \"\"\" 10.0 20.0 30.0 [torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 3] \"\"\" t . stride () # (0, 1)","title":"Broadcasting Vectors onto a matrix"},{"location":"lesson-11/A4-broadcasting/#about-1d-vectors-expanding-dimensions","text":"There is a distinction between the following: - tensor size(3) - tensor size(1, 3) - tensor size(3, 1) - This distinction will be essential to master : matrix to matrix operations vec = torch . tensor ([ 10. , 20. , 30. ]) vec . shape \"\"\" torch.Size([3]) \"\"\" How to convert a vector into a single row matrix vec_adj = vec . unsqueeze ( 0 ) vec_adj . shape \"\"\" torch.Size([1, 3]) \"\"\" vec_adj = vec [ None , :] vec_adj . shape \"\"\" torch.Size([1, 3]) \"\"\" vec_adj \"\"\" tensor([[10., 20., 30.]]) \"\"\" How to convert a vector into a single column matrix vec_adj = vec . unsqueeze ( 1 ) vec_adj . shape \"\"\" torch.Size([3, 1]) \"\"\" vec_adj = vec [:, None ] vec_adj . shape \"\"\" torch.Size([3, 1]) \"\"\" vec_adj \"\"\" tensor([[10.], [20.], [30.]]) \"\"\"","title":"About 1D Vectors + expanding dimensions"},{"location":"lesson-11/A4-broadcasting/#note-trailing-dimensions","text":"torch has a specific syntax to skip dimensions with ... A quick example: some_tensor = torch . randn ( 2 , 3 , 4 ) some_tensor . shape # torch.Size([2, 3, 4]) some_tensor [ ... , None ] . shape # torch.Size([2, 3, 4, 1]) some_tensor [ None , ... ] . shape # torch.Size([1, 2, 3, 4])","title":"Note: trailing dimensions"},{"location":"lesson-11/A4-broadcasting/#how-to-control-the-broadcasting-dimension","text":"since we combining a size(3) and a size(3, 3) , there's a question of if its doing it row-wise or column wise. This can be controlled by adjusting the vector ahead of time. vec = torch . tensor ([ 10. , 20. , 30. ]) mtx = torch . tensor ( range ( 1 , 10 )) mtx = mtx . reshape ( 3 , 3 ) Lets look at the default: vec + mtx \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" Now lets expand the dimensions of the vector vec [ None , ... ] + mtx # (1, 3) \"\"\" tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) \"\"\" vec [ ... , None ] + mtx # (3, 1) \"\"\" tensor([[11., 12., 13.], [24., 25., 26.], [37., 38., 39.]]) \"\"\" Expanding the first dimension, applies the vector row wise Expanding the last dimension applies the vector column wise","title":"How to control the broadcasting dimension"},{"location":"lesson-11/A4-broadcasting/#other-broadcasing-examples","text":"vec [ None , :] # torch.Size([1, 3]) vec [:, None ] #","title":"Other Broadcasing Examples"},{"location":"lesson-11/A5-more-broadcasting/","text":"Other Broadcasing Examples import torch from torch import tensor vec = torch . tensor ([ 10. , 20. , 30. ]) vec [ None , :] \"\"\" tensor([[10., 20., 30.]]) torch.Size([1, 3]) \"\"\" vec [:, None ] \"\"\" tensor([[10.], [20.], [30.]]) torch.Size([3, 1]) \"\"\" What happens when a column is multiplied against the row? vec [:, None ] * vec [ None , :] \"\"\" tensor([[100., 200., 300.], [200., 400., 600.], [300., 600., 900.]]) torch.Size([3, 1]) x torch.Size([1, 3]) => torch.Size([3, 3]) \"\"\" What if the order was reversed? vec [ None , :] * vec [:, None ] \"\"\" tensor([[100., 200., 300.], [200., 400., 600.], [300., 600., 900.]]) torch.Size([3, 1]) x torch.Size([1, 3]) => torch.Size([3, 3]) \"\"\" From the notebook: When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size Arrays do not need to have the same number of dimensions. For example, if you have a `256*256*3` array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: Image (3d array): 256 x 256 x 3 Scale (1d array): 3 Result (3d array): 256 x 256 x 3 The [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules) includes several examples of what dimensions can and can not be broadcast together.","title":"More Broadcasting"},{"location":"lesson-11/A5-more-broadcasting/#other-broadcasing-examples","text":"import torch from torch import tensor vec = torch . tensor ([ 10. , 20. , 30. ]) vec [ None , :] \"\"\" tensor([[10., 20., 30.]]) torch.Size([1, 3]) \"\"\" vec [:, None ] \"\"\" tensor([[10.], [20.], [30.]]) torch.Size([3, 1]) \"\"\" What happens when a column is multiplied against the row? vec [:, None ] * vec [ None , :] \"\"\" tensor([[100., 200., 300.], [200., 400., 600.], [300., 600., 900.]]) torch.Size([3, 1]) x torch.Size([1, 3]) => torch.Size([3, 3]) \"\"\" What if the order was reversed? vec [ None , :] * vec [:, None ] \"\"\" tensor([[100., 200., 300.], [200., 400., 600.], [300., 600., 900.]]) torch.Size([3, 1]) x torch.Size([1, 3]) => torch.Size([3, 3]) \"\"\" From the notebook: When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size Arrays do not need to have the same number of dimensions. For example, if you have a `256*256*3` array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: Image (3d array): 256 x 256 x 3 Scale (1d array): 3 Result (3d array): 256 x 256 x 3 The [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules) includes several examples of what dimensions can and can not be broadcast together.","title":"Other Broadcasing Examples"},{"location":"lesson-11/A6-matmul-broadcasting/","text":"Matmul with broadcasting From before, here's all the functions we have collected import torch from numba import njit from torch import tensor def matmul ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] return output @njit def dot ( a , b ): res = 0. for i in range ( len ( a )): res += a [ i ] * b [ i ] return res def matmul_numba ( A , B ): \"\"\"Note, should convert to numpy before giving to this function\"\"\" A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the numba-powered array calculation output [ i , j ] = dot ( A [ i , :]), B [:, j ] return output def matmul_tensor ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the tensor-powered array calculation output [ i , j ] = torch . dot ( A [ i , :], B [:, j ]) return output For fun, this author decided to make a pure numba function: one modification, the output array needs to be allocated outside of the loop @njit def matmul_pure_numba ( A , B , output ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] return output Lets re-write the function again with broadcasting def matmul_broadcasting ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): output [ i ] = ( A [ i , :, None ] * B ) . sum ( dim = 0 ) return output Discussion output [ i ] = ( A [ i , :, None ] * B ) . sum ( dim = 0 ) A # (5 x 784) minibatch for an image A [ i , :, None ] # take the first image, and expand the last dimension # (784 x 1) B # (784 x 10) ( A [ i , :, None ] * B ) \"\"\" Because of the dimension expansion, A is broadcast against B, and the shape is retained (784 x 10) \"\"\" ( A [ i , :, None ] * B ) . sum ( dim = 0 ) \"\"\" Finally this is summed \"across rows (dim=0)\", resulting in 10 numbers \"\"\" Lets do some timing comparisons from timeit import timeit fake_img = torch . randn ( 5 , 784 ) # 5 was an arbitrary batch size weights = torch . randn ( 784 , 10 ) # 10 was the number of classes we are trying to predict %% timeit matmul ( fake_img , weights ) # 684 ms \u00b1 2.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_numba ( fake_img . numpy (), weights . numpy ()) # 358 \u00b5s \u00b1 37 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_tensor ( fake_img , weights ) # 693 \u00b5s \u00b1 944 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) %% timeit matmul_broadcasting ( fake_img , weights ) # 104 \u00b5s \u00b1 260 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) %% timeit output = np . zeros ( shape = [ fake_img . shape [ 0 ], weights . shape [ - 1 ]]) matmul_pure_numba ( fake_img . numpy (), weights . numpy (), output ) # 48.4 \u00b5s \u00b1 4.38 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)","title":"Matrix Multiplication Broadcasting"},{"location":"lesson-11/A6-matmul-broadcasting/#matmul-with-broadcasting","text":"From before, here's all the functions we have collected import torch from numba import njit from torch import tensor def matmul ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] return output @njit def dot ( a , b ): res = 0. for i in range ( len ( a )): res += a [ i ] * b [ i ] return res def matmul_numba ( A , B ): \"\"\"Note, should convert to numpy before giving to this function\"\"\" A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the numba-powered array calculation output [ i , j ] = dot ( A [ i , :]), B [:, j ] return output def matmul_tensor ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): for j in range ( B_cols ): # substitue the tensor-powered array calculation output [ i , j ] = torch . dot ( A [ i , :], B [:, j ]) return output For fun, this author decided to make a pure numba function: one modification, the output array needs to be allocated outside of the loop @njit def matmul_pure_numba ( A , B , output ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape for i in range ( A_rows ): for j in range ( B_cols ): for k in range ( A_cols ): # same as B_rows output [ i , j ] += A [ i , k ] * B [ k , j ] return output Lets re-write the function again with broadcasting def matmul_broadcasting ( A , B ): A_rows , A_cols = A . shape B_rows , B_cols = B . shape output = torch . zeros ( A_rows , B_cols ) for i in range ( A_rows ): output [ i ] = ( A [ i , :, None ] * B ) . sum ( dim = 0 ) return output","title":"Matmul with broadcasting"},{"location":"lesson-11/A6-matmul-broadcasting/#discussion","text":"output [ i ] = ( A [ i , :, None ] * B ) . sum ( dim = 0 ) A # (5 x 784) minibatch for an image A [ i , :, None ] # take the first image, and expand the last dimension # (784 x 1) B # (784 x 10) ( A [ i , :, None ] * B ) \"\"\" Because of the dimension expansion, A is broadcast against B, and the shape is retained (784 x 10) \"\"\" ( A [ i , :, None ] * B ) . sum ( dim = 0 ) \"\"\" Finally this is summed \"across rows (dim=0)\", resulting in 10 numbers \"\"\" Lets do some timing comparisons from timeit import timeit fake_img = torch . randn ( 5 , 784 ) # 5 was an arbitrary batch size weights = torch . randn ( 784 , 10 ) # 10 was the number of classes we are trying to predict %% timeit matmul ( fake_img , weights ) # 684 ms \u00b1 2.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_numba ( fake_img . numpy (), weights . numpy ()) # 358 \u00b5s \u00b1 37 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit matmul_tensor ( fake_img , weights ) # 693 \u00b5s \u00b1 944 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each) %% timeit matmul_broadcasting ( fake_img , weights ) # 104 \u00b5s \u00b1 260 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each) %% timeit output = np . zeros ( shape = [ fake_img . shape [ 0 ], weights . shape [ - 1 ]]) matmul_pure_numba ( fake_img . numpy (), weights . numpy (), output ) # 48.4 \u00b5s \u00b1 4.38 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)","title":"Discussion"},{"location":"lesson-12/A1-intro/","text":"Introduction Remarks https://huggingface.co/spaces/pharma/CLIP-Interrogator Put an image, and it will generate a CLIP prompt. The CLIP prompt is intended to be supplied into -- hand written notes -- Looking at the Hugging Face App code: https://huggingface.co/spaces/pharma/CLIP-Interrogator/blob/main/app.py Note the reference data: https://huggingface.co/spaces/pharma/CLIP-Interrogator/tree/main/data The app will mix and match the various phrases to generate captions based on the uploaded image https://huggingface.co/spaces/pharma/CLIP-Interrogator/blob/main/data/artists.txt A. B. Jackson A. J. Casson A. R. Middleton Todd A.B. Frost A.D.M. Cooper Aaron Bohrod Aaron Douglas Aaron Jasinski Aaron Miller Aaron Nagel BLIP Language Model Bootstrapping Language Image Pre-training for Unified Vision-Language understanding arxiv paper This is specifically designed to give an ok caption from an image, but it is NOT the inverse of the CLIP encoder","title":"Intro"},{"location":"lesson-12/A1-intro/#introduction-remarks","text":"https://huggingface.co/spaces/pharma/CLIP-Interrogator Put an image, and it will generate a CLIP prompt. The CLIP prompt is intended to be supplied into -- hand written notes --","title":"Introduction Remarks"},{"location":"lesson-12/A1-intro/#looking-at-the-hugging-face-app-code","text":"https://huggingface.co/spaces/pharma/CLIP-Interrogator/blob/main/app.py Note the reference data: https://huggingface.co/spaces/pharma/CLIP-Interrogator/tree/main/data The app will mix and match the various phrases to generate captions based on the uploaded image https://huggingface.co/spaces/pharma/CLIP-Interrogator/blob/main/data/artists.txt A. B. Jackson A. J. Casson A. R. Middleton Todd A.B. Frost A.D.M. Cooper Aaron Bohrod Aaron Douglas Aaron Jasinski Aaron Miller Aaron Nagel","title":"Looking at the Hugging Face App code:"},{"location":"lesson-12/A1-intro/#blip-language-model","text":"Bootstrapping Language Image Pre-training for Unified Vision-Language understanding arxiv paper This is specifically designed to give an ok caption from an image, but it is NOT the inverse of the CLIP encoder","title":"BLIP Language Model"},{"location":"lesson-12/A2-matmul/","text":"Finishing up Matmul Recap The goal of this walkthrough is to remove as many loops as possible: Einstein Summation Einsten summation or torch.einsum is a compact representation for combining products and sums in a general way. So consider the following, we have two matrices: m1 and m2 m1 is 5 x 784 which represents the 5 images of 28 x 28 m2 is 784 x 10 which represents the weights, each one of those pixels relates to the 10 classes (0-9) digits import torch mr = torch . einsum ( 'ik,kj->ikj' , m1 , m2 ) mr . shape >>> torch . Size ([ 5 , 784 , 10 ]) This can also be thought of as this: m1, row1 x m2, col1 -> 784 elements m1, row2 x m2, col1 -> 784 elements ... and so on Resulting in a stack of 5 x 784 x 10 Consider the following: the first row of m1 times the first column of m2 first_by_first = ( m1 [ 0 ] * m2 [:, 0 ]) first_by_first . sum () >>> tensor ( - 10.94 ) As proof, the first sample in the result mr looks like the following: mr ( 1 ) tensor ([[ - 10.94 , - 0.68 , - 7.00 , - 4.01 , - 2.09 , - 3.36 , 3.91 , - 3.44 , - 11.47 , - 2.12 ], [ 14.54 , 6.00 , 2.89 , - 4.08 , 6.59 , - 14.74 , - 9.28 , 2.16 , - 15.28 , - 2.68 ], [ 2.22 , - 3.22 , - 4.80 , - 6.05 , 14.17 , - 8.98 , - 4.79 , - 5.44 , - 20.68 , 13.57 ], [ - 6.71 , 8.90 , - 7.46 , - 7.90 , 2.70 , - 4.73 , - 11.03 , - 12.98 , - 6.44 , 3.64 ], [ - 2.44 , - 6.40 , - 2.40 , - 9.04 , 11.18 , - 5.77 , - 8.92 , - 3.79 , - 8.98 , 5.28 ]]) And that upper left hand value matches or row x column multiplication So to recap: torch . einsum ( 'ik,kj->ikj' , m1 , m2 ) \"\"\" Will leave the elements in the middle (k) \"\"\" torch . einsum ( 'ik,kj->ij' , m1 , m2 ) \"\"\" Since the middle indice is not available, will sum in that dimension \"\"\" Using this we can re-write matmul using einsum def matmul ( a , b ): return torch . einsum ( 'ik,kj->ij' , a , b ) What about speed? test_close ( tr , matmul ( x_train , weights ), eps = 1e-3 ) % timeit - n 5 _ = matmul ( x_train , weights ) >>> 15.1 ms \u00b1 176 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 5 loops each ) Pytorch Matmul Speed test_close ( tr , x_train @ weights , eps = 1e-3 ) % timeit - n 5 _ = torch . matmul ( x_train , weights ) >>> 15.2 ms \u00b1 96.2 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 5 loops each ) What about GPU + CUDA Matmul? GPUs generally have lower clock speeds but are great at parallel calculations. To help illustrate this, consider the following function that only fills in one part of the grid (similar to first_by_first idea above) # the previous result of matmul tensor ([[ - 10.94 , - 0.68 , - 7.00 , - 4.01 , - 2.09 , - 3.36 , 3.91 , - 3.44 , - 11.47 , - 2.12 ], [ 14.54 , 6.00 , 2.89 , - 4.08 , 6.59 , - 14.74 , - 9.28 , 2.16 , - 15.28 , - 2.68 ], [ 2.22 , - 3.22 , - 4.80 , - 6.05 , 14.17 , - 8.98 , - 4.79 , - 5.44 , - 20.68 , 13.57 ], [ - 6.71 , 8.90 , - 7.46 , - 7.90 , 2.70 , - 4.73 , - 11.03 , - 12.98 , - 6.44 , 3.64 ], [ - 2.44 , - 6.40 , - 2.40 , - 9.04 , 11.18 , - 5.77 , - 8.92 , - 3.79 , - 8.98 , 5.28 ]]) Below is the function design just to calculate 1 value in the above array def matmul ( grid , a , b , c ): \"\"\" grid: a tuple of (x,y) coordinates a: tensor to multiply b: tensor to multiply c: a empty tensor to hold results \"\"\" i , j = grid if i < c . shape [ 0 ] and j < c . shape [ 1 ]: tmp = 0. for k in range ( a . shape [ 1 ]): tmp += a [ i , k ] * b [ k , j ] c [ i , j ] = tmp Below is the pattern to use the \"coordinate calculation function\" # results holder res = torch . zeros ( ar , bc ) # 0,0 will be the upper left value to calculat matmul (( 0 , 0 ), m1 , m2 , res ) Which results in tensor ([[ - 10.94 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ]]) So because this is localized (only writes to one section of the result), this can considered a kernel . The below is a general framework to loop through + apply a kernel or our function. def launch_kernel ( kernel , grid_x , grid_y , * args , ** kwargs ): for i in range ( grid_x ): for j in range ( grid_y ): kernel (( i , j ), * args , ** kwargs ) res = torch . zeros ( ar , bc ) launch_kernel ( matmul , ar , bc , m1 , m2 , res ) res tensor ([[ - 10.94 , - 0.68 , - 7.00 , - 4.01 , - 2.09 , - 3.36 , 3.91 , - 3.44 , - 11.47 , - 2.12 ], [ 14.54 , 6.00 , 2.89 , - 4.08 , 6.59 , - 14.74 , - 9.28 , 2.16 , - 15.28 , - 2.68 ], [ 2.22 , - 3.22 , - 4.80 , - 6.05 , 14.17 , - 8.98 , - 4.79 , - 5.44 , - 20.68 , 13.57 ], [ - 6.71 , 8.90 , - 7.46 , - 7.90 , 2.70 , - 4.73 , - 11.03 , - 12.98 , - 6.44 , 3.64 ], [ - 2.44 , - 6.40 , - 2.40 , - 9.04 , 11.18 , - 5.77 , - 8.92 , - 3.79 , - 8.98 , 5.28 ]]) But it is not fast, because the above is base python. How can this be written in cuda or gpu-language? numba has tools to actual generate cuda code, but it comes with some alterations. from numba import cuda @cuda . jit # compiles to GPU code def matmul ( a , b , c ): i , j = cuda . grid ( 2 ) if i < c . shape [ 0 ] and j < c . shape [ 1 ]: tmp = 0. for k in range ( a . shape [ 1 ]): tmp += a [ i , k ] * b [ k , j ] c [ i , j ] = tmp So lets try it out! The important step will be copying the data over to the GPU (by default it is sitting on the CPU) # will be our results holder on the GPU r = np . zeros ( tr . shape ) m1g = cuda . to_device ( x_train ) m2g = cuda . to_device ( weights ) rg = cuda . to_device ( r ) # or m1g , m2g , rg = map ( cuda . to_device , ( x_train , weights , r )) In cuda , there is a concept called TPB (or threads per block) # turns grid into blocks TPB = 16 rr , rc = r . shape # (3125, 1) blockspergrid = ( math . ceil ( rr / TPB ), math . ceil ( rc / TPB )) # this is our Cuda version of the function, along with additional params in [] matmul [ blockspergrid , ( TPB , TPB )]( m1g , m2g , rg ) # copy the results back to the CPU r = rg . copy_to_host () test_close ( tr , r , eps = 1e-3 ) Doing a quick speed benchmark %% timeit - n 10 matmul [ blockspergrid , ( TPB , TPB )]( m1g , m2g , rg ) r = rg . copy_to_host () >>> 3.61 ms \u00b1 708 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 10 loops each ) Which is already much faster, lets try out the native torch methods for using the GPU m1c = x_train . cuda () m2c = weights . cuda () # do the multiplication then copy back to cpu r = ( m1c @ m2c ) . cpu () % timeit - n 10 r = ( m1c @m2c ) . cpu () >> 458 \u00b5s \u00b1 93.1 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 10 loops each ) 0.5ms compared to the previous 500ms is a HUGE improvement.","title":"Matmul"},{"location":"lesson-12/A2-matmul/#finishing-up-matmul","text":"","title":"Finishing up Matmul"},{"location":"lesson-12/A2-matmul/#recap","text":"The goal of this walkthrough is to remove as many loops as possible:","title":"Recap"},{"location":"lesson-12/A2-matmul/#einstein-summation","text":"Einsten summation or torch.einsum is a compact representation for combining products and sums in a general way. So consider the following, we have two matrices: m1 and m2 m1 is 5 x 784 which represents the 5 images of 28 x 28 m2 is 784 x 10 which represents the weights, each one of those pixels relates to the 10 classes (0-9) digits import torch mr = torch . einsum ( 'ik,kj->ikj' , m1 , m2 ) mr . shape >>> torch . Size ([ 5 , 784 , 10 ]) This can also be thought of as this: m1, row1 x m2, col1 -> 784 elements m1, row2 x m2, col1 -> 784 elements ... and so on Resulting in a stack of 5 x 784 x 10 Consider the following: the first row of m1 times the first column of m2 first_by_first = ( m1 [ 0 ] * m2 [:, 0 ]) first_by_first . sum () >>> tensor ( - 10.94 ) As proof, the first sample in the result mr looks like the following: mr ( 1 ) tensor ([[ - 10.94 , - 0.68 , - 7.00 , - 4.01 , - 2.09 , - 3.36 , 3.91 , - 3.44 , - 11.47 , - 2.12 ], [ 14.54 , 6.00 , 2.89 , - 4.08 , 6.59 , - 14.74 , - 9.28 , 2.16 , - 15.28 , - 2.68 ], [ 2.22 , - 3.22 , - 4.80 , - 6.05 , 14.17 , - 8.98 , - 4.79 , - 5.44 , - 20.68 , 13.57 ], [ - 6.71 , 8.90 , - 7.46 , - 7.90 , 2.70 , - 4.73 , - 11.03 , - 12.98 , - 6.44 , 3.64 ], [ - 2.44 , - 6.40 , - 2.40 , - 9.04 , 11.18 , - 5.77 , - 8.92 , - 3.79 , - 8.98 , 5.28 ]]) And that upper left hand value matches or row x column multiplication So to recap: torch . einsum ( 'ik,kj->ikj' , m1 , m2 ) \"\"\" Will leave the elements in the middle (k) \"\"\" torch . einsum ( 'ik,kj->ij' , m1 , m2 ) \"\"\" Since the middle indice is not available, will sum in that dimension \"\"\" Using this we can re-write matmul using einsum def matmul ( a , b ): return torch . einsum ( 'ik,kj->ij' , a , b ) What about speed? test_close ( tr , matmul ( x_train , weights ), eps = 1e-3 ) % timeit - n 5 _ = matmul ( x_train , weights ) >>> 15.1 ms \u00b1 176 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 5 loops each )","title":"Einstein Summation"},{"location":"lesson-12/A2-matmul/#pytorch-matmul-speed","text":"test_close ( tr , x_train @ weights , eps = 1e-3 ) % timeit - n 5 _ = torch . matmul ( x_train , weights ) >>> 15.2 ms \u00b1 96.2 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 5 loops each )","title":"Pytorch Matmul Speed"},{"location":"lesson-12/A2-matmul/#what-about-gpu-cuda-matmul","text":"GPUs generally have lower clock speeds but are great at parallel calculations. To help illustrate this, consider the following function that only fills in one part of the grid (similar to first_by_first idea above) # the previous result of matmul tensor ([[ - 10.94 , - 0.68 , - 7.00 , - 4.01 , - 2.09 , - 3.36 , 3.91 , - 3.44 , - 11.47 , - 2.12 ], [ 14.54 , 6.00 , 2.89 , - 4.08 , 6.59 , - 14.74 , - 9.28 , 2.16 , - 15.28 , - 2.68 ], [ 2.22 , - 3.22 , - 4.80 , - 6.05 , 14.17 , - 8.98 , - 4.79 , - 5.44 , - 20.68 , 13.57 ], [ - 6.71 , 8.90 , - 7.46 , - 7.90 , 2.70 , - 4.73 , - 11.03 , - 12.98 , - 6.44 , 3.64 ], [ - 2.44 , - 6.40 , - 2.40 , - 9.04 , 11.18 , - 5.77 , - 8.92 , - 3.79 , - 8.98 , 5.28 ]]) Below is the function design just to calculate 1 value in the above array def matmul ( grid , a , b , c ): \"\"\" grid: a tuple of (x,y) coordinates a: tensor to multiply b: tensor to multiply c: a empty tensor to hold results \"\"\" i , j = grid if i < c . shape [ 0 ] and j < c . shape [ 1 ]: tmp = 0. for k in range ( a . shape [ 1 ]): tmp += a [ i , k ] * b [ k , j ] c [ i , j ] = tmp Below is the pattern to use the \"coordinate calculation function\" # results holder res = torch . zeros ( ar , bc ) # 0,0 will be the upper left value to calculat matmul (( 0 , 0 ), m1 , m2 , res ) Which results in tensor ([[ - 10.94 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ], [ 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 , 0.00 ]]) So because this is localized (only writes to one section of the result), this can considered a kernel . The below is a general framework to loop through + apply a kernel or our function. def launch_kernel ( kernel , grid_x , grid_y , * args , ** kwargs ): for i in range ( grid_x ): for j in range ( grid_y ): kernel (( i , j ), * args , ** kwargs ) res = torch . zeros ( ar , bc ) launch_kernel ( matmul , ar , bc , m1 , m2 , res ) res tensor ([[ - 10.94 , - 0.68 , - 7.00 , - 4.01 , - 2.09 , - 3.36 , 3.91 , - 3.44 , - 11.47 , - 2.12 ], [ 14.54 , 6.00 , 2.89 , - 4.08 , 6.59 , - 14.74 , - 9.28 , 2.16 , - 15.28 , - 2.68 ], [ 2.22 , - 3.22 , - 4.80 , - 6.05 , 14.17 , - 8.98 , - 4.79 , - 5.44 , - 20.68 , 13.57 ], [ - 6.71 , 8.90 , - 7.46 , - 7.90 , 2.70 , - 4.73 , - 11.03 , - 12.98 , - 6.44 , 3.64 ], [ - 2.44 , - 6.40 , - 2.40 , - 9.04 , 11.18 , - 5.77 , - 8.92 , - 3.79 , - 8.98 , 5.28 ]]) But it is not fast, because the above is base python. How can this be written in cuda or gpu-language? numba has tools to actual generate cuda code, but it comes with some alterations. from numba import cuda @cuda . jit # compiles to GPU code def matmul ( a , b , c ): i , j = cuda . grid ( 2 ) if i < c . shape [ 0 ] and j < c . shape [ 1 ]: tmp = 0. for k in range ( a . shape [ 1 ]): tmp += a [ i , k ] * b [ k , j ] c [ i , j ] = tmp So lets try it out! The important step will be copying the data over to the GPU (by default it is sitting on the CPU) # will be our results holder on the GPU r = np . zeros ( tr . shape ) m1g = cuda . to_device ( x_train ) m2g = cuda . to_device ( weights ) rg = cuda . to_device ( r ) # or m1g , m2g , rg = map ( cuda . to_device , ( x_train , weights , r )) In cuda , there is a concept called TPB (or threads per block) # turns grid into blocks TPB = 16 rr , rc = r . shape # (3125, 1) blockspergrid = ( math . ceil ( rr / TPB ), math . ceil ( rc / TPB )) # this is our Cuda version of the function, along with additional params in [] matmul [ blockspergrid , ( TPB , TPB )]( m1g , m2g , rg ) # copy the results back to the CPU r = rg . copy_to_host () test_close ( tr , r , eps = 1e-3 ) Doing a quick speed benchmark %% timeit - n 10 matmul [ blockspergrid , ( TPB , TPB )]( m1g , m2g , rg ) r = rg . copy_to_host () >>> 3.61 ms \u00b1 708 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 10 loops each ) Which is already much faster, lets try out the native torch methods for using the GPU m1c = x_train . cuda () m2c = weights . cuda () # do the multiplication then copy back to cpu r = ( m1c @ m2c ) . cpu () % timeit - n 10 r = ( m1c @m2c ) . cpu () >> 458 \u00b5s \u00b1 93.1 \u00b5s per loop ( mean \u00b1 std . dev . of 7 runs , 10 loops each ) 0.5ms compared to the previous 500ms is a HUGE improvement.","title":"What about GPU + CUDA Matmul?"},{"location":"lesson-12/A3-mean-shift/","text":"Practice multiplying matrices Goal: practice tensor manipulation, matrix changes + multiplication are the BASICS. All algorithms build on these addition, multiplication + broadcasting ideas Clustering + Mean Shift Algorithms What is cluster analysis? It is unsupervised analysis, \"are there groups of similar things\"? Cluster analysis can be over or mis-used. General guidance: same things (pixels in images) same scales (same 1 - 5 range for surveys) First create synthetic data with intentional clusters For now, will generate 6 centroids with 250 samples each import math , matplotlib.pyplot as plt , operator , torch from functools import partial from torch.distributions.multivariate_normal import MultivariateNormal from torch import tensor torch . manual_seed ( 42 ) torch . set_printoptions ( precision = 3 , linewidth = 140 , sci_mode = False ) n_clusters = 6 n_samples = 250 # generates 6 points in 2 dimensions (x, y) for plotting # the 70 will create a spread, and the -35 is the offset centroids = torch . rand ( n_clusters , 2 ) * 70 - 35 def sample ( m ): \"\"\" m = (x, y) the covariance matrix looks like: tensor([[5., 0.], [0., 5.]]) \"\"\" covar_mtx = torch . diag ( tensor ([ 5. , 5. ])) return MultivariateNormal ( m , covar_mtx ) . sample (( n_samples ,)) # adding groups of points slices = [ sample ( c ) for c in centroids ] data = torch . cat ( slices ) Writing a plotting function def plot_data ( centroids , data , n_samples , ax = None ): if ax is None : _ , ax = plt . subplots () for i , centroid in enumerate ( centroids ): samples = data [ i * n_samples :( i + 1 ) * n_samples ] ax . scatter ( samples [:, 0 ], samples [:, 1 ], s = 1 ) ax . plot ( * centroid , markersize = 10 , marker = \"x\" , color = 'k' , mew = 5 ) ax . plot ( * centroid , markersize = 5 , marker = \"x\" , color = 'm' , mew = 2 ) plot_data(centroids, data, n_samples) Now even though clusters are intentionally there, there are no labels! It's simply a collection of scatterplot data. Mean Shift mean-shift does not require the number of clusters to be known before hand, like k-means . Algorithm Start with a point x out of dataset X (big X) Find the distance between x and all other remaining points Take a weighted average determined by closeness to X will use gaussian kernel the farther points should have lower contribution, the rate of this decay is the bandwidth or the standard deviation of the gaussian the closer points should have larger contribution Update x as a weighted average or all other points X so all points x will be calculated on last step's positions Writing it in pytorch Will push points closer and closer together, similar to gravity def gaussian ( d , bw ): \"\"\"also known as the normal distribution\"\"\" numer = torch . exp ( - 0.5 * (( d / bw )) ** 2 ) denom = ( bw * math . sqrt ( 2 * math . pi )) return numer / denom This is a high-level plotting function that actually accepts another function from functools import partial def plot_func ( f ): \"\"\" f - some function to be plotting from 0, 10, with 100 steps \"\"\" x = torch . linspace ( 0 , 10 , 100 ) plt . plot ( x , f ( x )) # pass the gaussian with a bandwidth of 2.5 plot_func ( partial ( gaussian , bw = 2.5 )) # or plot_func ( lambda x : gaussian ( x , bw = 2.5 )) Note : Choose a bandwidth covers about 1/3rd of the data # keep the orignal data # torch.Size(1500, 2) Big_X = data . clone () # isolate one point # torch.Size(2) little_x = data [ 0 ] Sample subtraction ( little_x - big_X )[: 8 ] >>> tensor ([[ 0.000 , 0.000 ], [ 0.513 , - 3.865 ], [ - 4.227 , - 2.345 ], [ 0.557 , - 3.685 ], [ - 5.033 , - 3.745 ], [ - 4.073 , - 0.638 ], [ - 3.415 , - 5.601 ], [ - 1.920 , - 5.686 ]]) Calculate the Euclidean distance euclid_distance = (( little_x - Big_X ) ** 2 ) . sum ( 1 ) . sqrt () # pass weights to gaussian function, torch.Size(1500) weight = gaussian ( euclid_distance , 2.5 ) # then we want to weight the original, since this is matrix multiplication, the weight needs # an additional dimension # [1500, 1] x [1500, 2] weighted_coords = weight [:, None ] * big_X # then perform average as normal new_location = weighted_coords . sum ( dim = 0 ) / weight . sum () Lets write this in a convenience function def one_update ( X ): \"\"\" updates all points, note that this updating in place, the first point will be updated + will affect future updates \"\"\" # iterating through each point for i , x in enumerate ( X ): # calculate distance from 1 point vs. all others dist = torch . sqrt ((( x - X ) ** 2 ) . sum ( 1 )) # calculate gaussian weighting weight = gaussian ( dist , 2.5 ) # update the point with the new average X [ i ] = ( weight [:, None ] * X ) . sum ( 0 ) / weight . sum () def meanshift ( data ): X = data . clone () # go through 5 iterations for it in range ( 5 ): one_update ( X ) return X A quick time benchmark % time X = meanshift ( data ) >>> CPU times : user 642 ms , sys : 0 ns , total : 642 ms plot_data(centroids+2, X, n_samples) Write the algo for the GPU Looking again at the main update function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def one_update ( X ): \"\"\" updates all points, note that this updating in place, the first point will be updated + will affect future updates \"\"\" # iterating through each point for i , x in enumerate ( X ): # calculate distance from 1 point vs. all others dist = torch . sqrt ((( x - X ) ** 2 ) . sum ( 1 )) # calculate gaussian weighting weight = gaussian ( dist , 2.5 ) # update the point with the new average X [ i ] = ( weight [:, None ] * X ) . sum ( 0 ) / weight . sum () The strategy to reduce the overhead of row looping (line 7) will be broadcasting. To avoid memory allocation issues, this will be mini-batched, meaning a few records will be broadcast at a time. # batch size bs = 5 # 1500 x 2 X = data . clone () # now represents a subset of points instead of a single # 5 x 2 x = X [: bs ] Now the distance calculation becomes more difficult: [5 x 2] [1500 x 2] The output needs to be [5 x 1500 x 2] So whats the strategy? [5 x 1 x 2] [1 x 1500 x 2] The output should be [5 x 1500 x 2] this is happening because the 1's added to the dimensions result in broadcasting Walking through it slowly with shapes # torch.Size([5, 1500, 2]) delta = X [ None , :] - x [:, None ] # summing on the LAST dimension # torch.Size([5, 1500]) sq_dist = ( delta ** 2 ) . sum ( dim = 2 ) # squares in place, still: torch.Size([5, 1500]) dist = sq_dist . sqrt () Now wrap in a function def dist_b ( a , b ): delta = a [ None , :] - b [:, None ] sq_dist = ( delta ** 2 ) . sum ( 2 ) return sq_dist . sqrt () # note that its important which slot big X and little x go into dist_b ( X , x ) >>> tensor ([[ 0.000 , 3.899 , 4.834 , ... , 17.628 , 22.610 , 21.617 ], [ 3.899 , 0.000 , 4.978 , ... , 21.499 , 26.508 , 25.500 ], [ 4.834 , 4.978 , 0.000 , ... , 19.373 , 24.757 , 23.396 ], [ 3.726 , 0.185 , 4.969 , ... , 21.335 , 26.336 , 25.333 ], [ 6.273 , 5.547 , 1.615 , ... , 20.775 , 26.201 , 24.785 ]]) Once this is gathered, applied the gaussian norm weight = gaussian ( dist_b ( X , x ), 2 ) Again the weight calculation weight is [5 x 1500] X is [1500 x 2] So the same strategy will be employed: weight to be [5, 1500, 1] X is [1, 1500, 2] Walking through slowly # torch.Size([5, 1500, 2]) weighted_coordinates = weight [ ... , None ] * X [ None ] # calculate the average in two parts # torch.Size([5, 2]) numer = weighted_coordinates . sum ( 1 ) # torch.Size(5, 1) denom = weight . sum ( 1 , keepdim = True ) updated_points = numer / denom Now writing in a gpu function: ```python linenum=\"1\" def meanshift(data, bs: int = 500): n = len(data) X = data.clone() # 5 iterations as before for it in range(5): # going through the dataset, by batch size for i in range(0, n, bs): # generates start + end indices s = slice(i, min(i + bs, n)) # does the recalculation here weight = gaussian(dist_b(X, X[s]), 2.5) div = weight.sum(1, keepdim=True) # finally the update for the same batch slice X[s] = weight @ X / div return X Timing python %timeit -n 5 _=meanshift(data, 1250).cpu() 2.52 ms \u00b1 272 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 5 loops each) ``` Homework Homework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version. \"LHS\" comes up a lot == locality sensitive hashing , for an optimized nearest neighbor Bonus: Implement it in APL too! Super bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time. Super super bonus: Publish a paper that describes it :D","title":"Implement Clustering with Matrices"},{"location":"lesson-12/A3-mean-shift/#practice-multiplying-matrices","text":"Goal: practice tensor manipulation, matrix changes + multiplication are the BASICS. All algorithms build on these addition, multiplication + broadcasting ideas","title":"Practice multiplying matrices"},{"location":"lesson-12/A3-mean-shift/#clustering-mean-shift-algorithms","text":"What is cluster analysis? It is unsupervised analysis, \"are there groups of similar things\"? Cluster analysis can be over or mis-used. General guidance: same things (pixels in images) same scales (same 1 - 5 range for surveys)","title":"Clustering + Mean Shift Algorithms"},{"location":"lesson-12/A3-mean-shift/#first-create-synthetic-data-with-intentional-clusters","text":"For now, will generate 6 centroids with 250 samples each import math , matplotlib.pyplot as plt , operator , torch from functools import partial from torch.distributions.multivariate_normal import MultivariateNormal from torch import tensor torch . manual_seed ( 42 ) torch . set_printoptions ( precision = 3 , linewidth = 140 , sci_mode = False ) n_clusters = 6 n_samples = 250 # generates 6 points in 2 dimensions (x, y) for plotting # the 70 will create a spread, and the -35 is the offset centroids = torch . rand ( n_clusters , 2 ) * 70 - 35 def sample ( m ): \"\"\" m = (x, y) the covariance matrix looks like: tensor([[5., 0.], [0., 5.]]) \"\"\" covar_mtx = torch . diag ( tensor ([ 5. , 5. ])) return MultivariateNormal ( m , covar_mtx ) . sample (( n_samples ,)) # adding groups of points slices = [ sample ( c ) for c in centroids ] data = torch . cat ( slices ) Writing a plotting function def plot_data ( centroids , data , n_samples , ax = None ): if ax is None : _ , ax = plt . subplots () for i , centroid in enumerate ( centroids ): samples = data [ i * n_samples :( i + 1 ) * n_samples ] ax . scatter ( samples [:, 0 ], samples [:, 1 ], s = 1 ) ax . plot ( * centroid , markersize = 10 , marker = \"x\" , color = 'k' , mew = 5 ) ax . plot ( * centroid , markersize = 5 , marker = \"x\" , color = 'm' , mew = 2 ) plot_data(centroids, data, n_samples) Now even though clusters are intentionally there, there are no labels! It's simply a collection of scatterplot data.","title":"First create synthetic data with intentional clusters"},{"location":"lesson-12/A3-mean-shift/#mean-shift","text":"mean-shift does not require the number of clusters to be known before hand, like k-means .","title":"Mean Shift"},{"location":"lesson-12/A3-mean-shift/#algorithm","text":"Start with a point x out of dataset X (big X) Find the distance between x and all other remaining points Take a weighted average determined by closeness to X will use gaussian kernel the farther points should have lower contribution, the rate of this decay is the bandwidth or the standard deviation of the gaussian the closer points should have larger contribution Update x as a weighted average or all other points X so all points x will be calculated on last step's positions","title":"Algorithm"},{"location":"lesson-12/A3-mean-shift/#writing-it-in-pytorch","text":"Will push points closer and closer together, similar to gravity def gaussian ( d , bw ): \"\"\"also known as the normal distribution\"\"\" numer = torch . exp ( - 0.5 * (( d / bw )) ** 2 ) denom = ( bw * math . sqrt ( 2 * math . pi )) return numer / denom This is a high-level plotting function that actually accepts another function from functools import partial def plot_func ( f ): \"\"\" f - some function to be plotting from 0, 10, with 100 steps \"\"\" x = torch . linspace ( 0 , 10 , 100 ) plt . plot ( x , f ( x )) # pass the gaussian with a bandwidth of 2.5 plot_func ( partial ( gaussian , bw = 2.5 )) # or plot_func ( lambda x : gaussian ( x , bw = 2.5 )) Note : Choose a bandwidth covers about 1/3rd of the data # keep the orignal data # torch.Size(1500, 2) Big_X = data . clone () # isolate one point # torch.Size(2) little_x = data [ 0 ] Sample subtraction ( little_x - big_X )[: 8 ] >>> tensor ([[ 0.000 , 0.000 ], [ 0.513 , - 3.865 ], [ - 4.227 , - 2.345 ], [ 0.557 , - 3.685 ], [ - 5.033 , - 3.745 ], [ - 4.073 , - 0.638 ], [ - 3.415 , - 5.601 ], [ - 1.920 , - 5.686 ]]) Calculate the Euclidean distance euclid_distance = (( little_x - Big_X ) ** 2 ) . sum ( 1 ) . sqrt () # pass weights to gaussian function, torch.Size(1500) weight = gaussian ( euclid_distance , 2.5 ) # then we want to weight the original, since this is matrix multiplication, the weight needs # an additional dimension # [1500, 1] x [1500, 2] weighted_coords = weight [:, None ] * big_X # then perform average as normal new_location = weighted_coords . sum ( dim = 0 ) / weight . sum () Lets write this in a convenience function def one_update ( X ): \"\"\" updates all points, note that this updating in place, the first point will be updated + will affect future updates \"\"\" # iterating through each point for i , x in enumerate ( X ): # calculate distance from 1 point vs. all others dist = torch . sqrt ((( x - X ) ** 2 ) . sum ( 1 )) # calculate gaussian weighting weight = gaussian ( dist , 2.5 ) # update the point with the new average X [ i ] = ( weight [:, None ] * X ) . sum ( 0 ) / weight . sum () def meanshift ( data ): X = data . clone () # go through 5 iterations for it in range ( 5 ): one_update ( X ) return X A quick time benchmark % time X = meanshift ( data ) >>> CPU times : user 642 ms , sys : 0 ns , total : 642 ms plot_data(centroids+2, X, n_samples)","title":"Writing it in pytorch"},{"location":"lesson-12/A3-mean-shift/#write-the-algo-for-the-gpu","text":"Looking again at the main update function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def one_update ( X ): \"\"\" updates all points, note that this updating in place, the first point will be updated + will affect future updates \"\"\" # iterating through each point for i , x in enumerate ( X ): # calculate distance from 1 point vs. all others dist = torch . sqrt ((( x - X ) ** 2 ) . sum ( 1 )) # calculate gaussian weighting weight = gaussian ( dist , 2.5 ) # update the point with the new average X [ i ] = ( weight [:, None ] * X ) . sum ( 0 ) / weight . sum () The strategy to reduce the overhead of row looping (line 7) will be broadcasting. To avoid memory allocation issues, this will be mini-batched, meaning a few records will be broadcast at a time. # batch size bs = 5 # 1500 x 2 X = data . clone () # now represents a subset of points instead of a single # 5 x 2 x = X [: bs ] Now the distance calculation becomes more difficult: [5 x 2] [1500 x 2] The output needs to be [5 x 1500 x 2] So whats the strategy? [5 x 1 x 2] [1 x 1500 x 2] The output should be [5 x 1500 x 2] this is happening because the 1's added to the dimensions result in broadcasting Walking through it slowly with shapes # torch.Size([5, 1500, 2]) delta = X [ None , :] - x [:, None ] # summing on the LAST dimension # torch.Size([5, 1500]) sq_dist = ( delta ** 2 ) . sum ( dim = 2 ) # squares in place, still: torch.Size([5, 1500]) dist = sq_dist . sqrt () Now wrap in a function def dist_b ( a , b ): delta = a [ None , :] - b [:, None ] sq_dist = ( delta ** 2 ) . sum ( 2 ) return sq_dist . sqrt () # note that its important which slot big X and little x go into dist_b ( X , x ) >>> tensor ([[ 0.000 , 3.899 , 4.834 , ... , 17.628 , 22.610 , 21.617 ], [ 3.899 , 0.000 , 4.978 , ... , 21.499 , 26.508 , 25.500 ], [ 4.834 , 4.978 , 0.000 , ... , 19.373 , 24.757 , 23.396 ], [ 3.726 , 0.185 , 4.969 , ... , 21.335 , 26.336 , 25.333 ], [ 6.273 , 5.547 , 1.615 , ... , 20.775 , 26.201 , 24.785 ]]) Once this is gathered, applied the gaussian norm weight = gaussian ( dist_b ( X , x ), 2 ) Again the weight calculation weight is [5 x 1500] X is [1500 x 2] So the same strategy will be employed: weight to be [5, 1500, 1] X is [1, 1500, 2] Walking through slowly # torch.Size([5, 1500, 2]) weighted_coordinates = weight [ ... , None ] * X [ None ] # calculate the average in two parts # torch.Size([5, 2]) numer = weighted_coordinates . sum ( 1 ) # torch.Size(5, 1) denom = weight . sum ( 1 , keepdim = True ) updated_points = numer / denom Now writing in a gpu function: ```python linenum=\"1\" def meanshift(data, bs: int = 500): n = len(data) X = data.clone() # 5 iterations as before for it in range(5): # going through the dataset, by batch size for i in range(0, n, bs): # generates start + end indices s = slice(i, min(i + bs, n)) # does the recalculation here weight = gaussian(dist_b(X, X[s]), 2.5) div = weight.sum(1, keepdim=True) # finally the update for the same batch slice X[s] = weight @ X / div return X Timing python %timeit -n 5 _=meanshift(data, 1250).cpu() 2.52 ms \u00b1 272 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 5 loops each) ```","title":"Write the algo for the GPU"},{"location":"lesson-12/A3-mean-shift/#homework","text":"Homework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version. \"LHS\" comes up a lot == locality sensitive hashing , for an optimized nearest neighbor Bonus: Implement it in APL too! Super bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time. Super super bonus: Publish a paper that describes it :D","title":"Homework"},{"location":"lesson-12/A4-calculus/","text":"Calculus -- Handwritten notes","title":"Intro to calculus"},{"location":"lesson-12/A4-calculus/#calculus","text":"-- Handwritten notes","title":"Calculus"}]}